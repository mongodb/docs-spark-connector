To write data to MongoDB, call the ``write()`` function on your
DataFrame object. This function returns a
`DataFrameWriter <https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameWriter.html#pyspark.sql.DataFrameWriter/>`__
object, which you can use to specify the format and other configuration settings for your
batch write operation. 

You must specify the following configuration settings to write to MongoDB:
         
.. list-table::
   :header-rows: 1
   :stub-columns: 1
   :widths: 10 40
         
   * - Setting
     - Description
         
   * - ``dataFrame.write.format()``
     - Specifies the format of the underlying output data source. Use ``mongodb``
       to write to MongoDB.
         
   * - ``dataFrame.write.option()``
     - Use the ``option`` method to configure batch write settings, including the
       MongoDB deployment
       :manual:`connection string </reference/connection-string/>`,
       MongoDB database and collection, and
       destination directory.

       For a list of batch write configuration options, see
       the :ref:`spark-batch-write-conf` guide.

The following code example creates a DataFrame from a list of tuples containing names
and ages and a list of column names. The example then writes this DataFrame to the
``people.contacts`` collection in MongoDB.

.. code-block:: python

   dataFrame = spark.createDataFrame([("Bilbo Baggins",  50), ("Gandalf", 1000), ("Thorin", 195), ("Balin", 178), ("Kili", 77),
      ("Dwalin", 169), ("Oin", 167), ("Gloin", 158), ("Fili", 82), ("Bombur", None)], ["name", "age"])

   dataFrame.write.format("mongodb")
                  .mode("append")
                  .option("database", "people")
                  .option("collection", "contacts")
                  .save()

.. include:: /includes/save-modes-tip.rst