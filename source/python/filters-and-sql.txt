===============
Filters and SQL
===============

.. default-domain:: mongodb

.. contents:: On this page
   :local:
   :backlinks: none
   :depth: 1
   :class: singlecol

Filters
-------

.. note::

   When using filters with DataFrames or the Python API, the
   underlying Mongo Connector code constructs an :manual:`aggregation
   pipeline </core/aggregation-pipeline/>` to filter the data in
   MongoDB before sending it to Spark.

Use ``filter()`` to read a subset of data from your MongoDB collection.

.. include:: /includes/example-load-dataframe.rst

First, set up a dataframe to connect with your default MongoDB data
source:

.. code-block:: python

   df = spark.read.format("mongo").load()

The following example includes only
records in which the ``qty`` field is greater than or equal to ``10``.

.. code-block:: python

   df.filter(df['qty'] >= 10).show()

The operation prints the following output:

.. code-block:: none

   +---+----+------+
   |_id| qty|  type|
   +---+----+------+
   |2.0|10.0|orange|
   |3.0|15.0|banana|
   +---+----+------+

SQL
---

Before you can run SQL queries against your DataFrame, you need to
register a temporary table.

The following example registers a temporary table called ``temp``,
then uses SQL to query for records in which the ``type`` field
contains the letter ``e``:

.. code-block:: python

   df.createOrReplaceTempView("temp")
   some_fruit = spark.sql("SELECT type, qty FROM temp WHERE type LIKE '%e%'")
   some_fruit.show()

In the ``pyspark`` shell, the operation prints the following output:

.. code-block:: none

   +------+----+
   |  type| qty|
   +------+----+
   | apple| 5.0|
   |orange|10.0|
   +------+----+
