You can create a Spark DataFrame to hold data from the MongoDB
collection specified in the
:ref:`spark.mongodb.read.connection.uri <pyspark-shell>` option which your
``SparkSession`` option is using.

.. include:: /includes/example-load-dataframe.rst

Assign the collection to a DataFrame with ``spark.read()``
from within the ``pyspark`` shell.

.. code-block:: python

   df = spark.read.format("mongodb").load()

Spark samples the records to infer the schema of the collection.

.. code-block:: python

   df.printSchema()

The above operation produces the following shell output:

.. code-block:: none

   root
    |-- _id: double (nullable = true)
    |-- qty: double (nullable = true)
    |-- type: string (nullable = true)

If you need to read from a different MongoDB collection,
use the ``.option`` method when reading data into a DataFrame.

To read from a collection called ``contacts`` in a database called
``people``, specify ``people.contacts`` in the input URI option.

.. code-block:: python

   df = spark.read.format("mongodb").option("uri", "mongodb://127.0.0.1/people.contacts").load()
