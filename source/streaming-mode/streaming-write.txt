.. _streaming-write-to-mongodb:

================
Write to MongoDB
================

.. toctree::
   :caption: Write Configuration Options

   /streaming-mode/streaming-write-config

.. tabs-selector:: drivers

.. tabs-drivers::

   tabs:

     - id: java-sync
       content: |

         Specify write stream configuration settings on your streaming 
         Dataset or streaming DataFrame using `DataStreamWriter <https://spark.apache.org/docs/latest/api/java/org/apache/spark/sql/streaming/DataStreamWriter.html>`__. You must specify the following configuration 
         settings to write to MongoDB:
         
         .. list-table::
            :header-rows: 1
            :stub-columns: 1
            :widths: 10 40
         
            * - Setting
              - Description
         
            * - ``writeStream.format()``
              - Specifies the format of the underlying output data source. Use ``mongodb``
                to write to MongoDB.
         
            * - ``writeStream.option()``
              - Use the ``option`` method to configure stream settings, including the
                MongoDB deployment
                :manual:`connection string </reference/connection-string/>`,
                MongoDB database and collection, and
                `checkpoint directory <https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#recovering-from-failures-with-checkpointing>`__.

                For a list of write stream configuration options, see
                the :ref:`spark-streaming-write-conf` guide.

            * - ``writeStream.outputMode()``
              - Specifies how data of a streaming DataFrame/Dataset is 
                written to a streaming sink. To view a list of all 
                supported output modes, see `the Java outputMode documentation <https://spark.apache.org/docs/latest/api/java/org/apache/spark/sql/streaming/DataStreamWriter.html#outputMode-java.lang.String->`__.
 
            * - ``writeStream.trigger()``
              - Specifies how often results should be written to the streaming sink. 
                
                To use continuous processing, pass ``Trigger.Continuous(<time value>)`` 
                as an argument, where ``<time value>`` is how often the Spark Connector 
                should asynchronously checkpoint. If you 
                pass any other static method of the ``Trigger`` class, or if you don't 
                call ``writeStream.trigger()``, the Spark connector will use 
                micro-batch processing instead. 
  
                To view a list of all supported processing policies, see `the Java 
                trigger documentation <https://spark.apache.org/docs/latest/api/java/org/apache/spark/sql/streaming/Trigger.html>`__.

                .. include:: /includes/note-trigger-method
        
         The following code snippet shows how to use the preceding 
         configuration settings to stream data to MongoDB:

         .. code-block:: java
            :copyable: true
         
            <streaming Dataset/DataFrame>.writeStream()
              .format("mongodb")
              .option("spark.mongodb.connection.uri", <mongodb-connection-string>)
              .option("spark.mongodb.database", <database-name>)
              .option("spark.mongodb.collection", <collection-name>)
              .outputMode("append");

         For a complete list of methods, see the 
         `Java Structured Streaming reference <https://spark.apache.org/docs/latest/api/java/org/apache/spark/sql/streaming/DataStreamWriter.html>`__. 

     - id: python
       content: |

         Specify write stream configuration settings on your streaming 
         DataFrame using `DataStreamWriter <https://spark.apache.org/docs/latest/api/python/reference/pyspark.ss/api/pyspark.sql.streaming.DataStreamWriter.html>`__. You 
         must specify the following configuration settings to write 
         to MongoDB:
         
         .. list-table::
            :header-rows: 1
            :stub-columns: 1
            :widths: 10 40
         
            * - Setting
              - Description
         
            * - ``writeStream.format()``
              - Specifies the format of the underlying output data source. Use ``mongodb``
                to write to MongoDB.
         
            * - ``writeStream.option()``
              - Use the ``option`` method to configure stream settings, including the
                MongoDB deployment
                :manual:`connection string </reference/connection-string/>`,
                MongoDB database and collection, and
                `checkpoint directory <https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#recovering-from-failures-with-checkpointing>`__.

                For a list of write stream configuration options, see
                the :ref:`spark-streaming-write-conf` guide.

            * - ``writeStream.outputMode()``
              - Specifies how data of a streaming DataFrame is 
                written to a streaming sink. To view a list of all 
                supported output modes, see `the pyspark outputMode documentation <https://spark.apache.org/docs/latest/api/python/reference/pyspark.ss/api/pyspark.sql.streaming.DataStreamWriter.outputMode.html#pyspark.sql.streaming.DataStreamWriter.outputMode>`__.

            * - ``writeStream.trigger()``
              - Specifies how often results should be 
                written to the streaming sink. 

                To use continuous processing, pass the method a time value 
                using the ``continuous`` parameter.
                If you pass any other named parameter, or if you don't 
                call ``writeStream.trigger()``, the Spark Connector will use 
                micro-batch processing instead. 
                
                To view a list of all supported processing policies, see 
                `the pyspark trigger documentation <https://spark.apache.org/docs/latest/api/python/reference/pyspark.ss/api/pyspark.sql.streaming.DataStreamWriter.trigger.html>`__.

                .. include:: /includes/note-trigger-method
         
         The following code snippet shows how to use the preceding 
         configuration settings to stream data to MongoDB:

         .. code-block:: python
            :copyable: true
         
            <streaming DataFrame>.writeStream \
              .format("mongodb") \
              .option("spark.mongodb.connection.uri", <mongodb-connection-string>) \
              .option("spark.mongodb.database", <database-name>) \
              .option("spark.mongodb.collection", <collection-name>) \
              .outputMode("append")
         
         For a complete list of methods, see the 
         `pyspark Structured Streaming reference <https://spark.apache.org/docs/latest/api/python/reference/pyspark.ss/api/pyspark.sql.streaming.DataStreamWriter.html>`__.

     - id: scala
       content: |

         Specify write stream configuration settings on your streaming 
         Dataset or streaming DataFrame using `DataStreamWriter <https://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/streaming/DataStreamWriter.html>`__. You must specify the following configuration 
         settings to write to MongoDB:
         
         .. list-table::
            :header-rows: 1
            :stub-columns: 1
            :widths: 10 40
         
            * - Setting
              - Description
         
            * - ``writeStream.format()``
              - Specifies the format of the underlying output data source. Use ``mongodb``
                to write to MongoDB.
         
            * - ``writeStream.option()``
              - Use the ``option`` method to configure stream settings, including the
                MongoDB deployment
                :manual:`connection string </reference/connection-string/>`,
                MongoDB database and collection, and
                `checkpoint directory <https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#recovering-from-failures-with-checkpointing>`__.

                For a list of write stream configuration options, see
                the :ref:`spark-streaming-write-conf` guide.

            * - ``writeStream.outputMode()``
              - Specifies how data of a streaming DataFrame/Dataset is 
                written to a streaming sink. To view a list of all 
                supported output modes, see `the Scala outputMode documentation <https://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/streaming/DataStreamWriter.html#outputMode(outputMode:String):org.apache.spark.sql.streaming.DataStreamWriter[T]>`__.
 
            * - ``writeStream.trigger()``
              - Specifies how often results should be 
                written to the streaming sink.

                To use continuous processing, pass ``Trigger.Continuous(<time value>)`` 
                as an argument, where ``<time value>`` is how often the Spark Connector 
                should asynchronously checkpoint. If you 
                pass any other static method of the ``Trigger`` class, or if you don't 
                call ``writeStream.trigger()``, the Spark connector will use 
                micro-batch processing instead. 
                
                To view a list of all 
                supported processing policies, see `the Scala trigger documentation <https://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/streaming/DataStreamWriter.html#trigger(trigger:org.apache.spark.sql.streaming.Trigger):org.apache.spark.sql.streaming.DataStreamWriter[T]>`__.

                .. include:: /includes/note-trigger-method
        
         The following code snippet shows how to use the preceding 
         configuration settings to stream data to MongoDB:

         .. code-block:: scala
            :copyable: true
         
            <streaming Dataset/DataFrame>.writeStream
              .format("mongodb")
              .option("spark.mongodb.connection.uri", <mongodb-connection-string>)
              .option("spark.mongodb.database", <database-name>)
              .option("spark.mongodb.collection", <collection-name>)
              .outputMode("append")
 
         For a complete list of methods, see the 
         `Scala Structured Streaming reference <https://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/streaming/DataStreamWriter.html>`__.

Example
-------

The following example shows how to stream data from a :abbr:`CSV (comma-separated values)`
file to MongoDB:

.. tabs-drivers::

   tabs:

     - id: java-sync
       content: |

         1. Create a 
            `DataStreamReader <https://spark.apache.org/docs/latest/api/java/org/apache/spark/sql/streaming/DataStreamReader.html>`__ 
            that reads from the CSV file.

         #. Create a 
            `DataStreamWriter <https://spark.apache.org/docs/latest/api/java/org/apache/spark/sql/streaming/DataStreamWriter.html>`__ 
            by calling the ``writeStream()`` method on the streaming 
            Dataset or streaming DataFrame that you created with a 
            ``DataStreamReader``. Specify the format ``mongodb`` using 
            the ``format()`` method.

         #. Call the ``start()`` method on the ``DataStreamWriter``
            instance to begin the stream.
         
         As the connector reads data from the CSV file, it adds that 
         data to MongoDB using the `outputMode <https://spark.apache.org/docs/latest/api/java/org/apache/spark/sql/streaming/DataStreamWriter.html#outputMode-java.lang.String->`__ 
         you specify.

         .. code-block:: java
            :copyable: true

            // create a local SparkSession
            SparkSession spark = SparkSession.builder()
              .appName("writeExample")
              .master("spark://spark-master:<port>")
              .config("spark.jars", "<mongo-spark-connector-JAR-file-name>")
              .getOrCreate();
             
            // define a streaming query
            DataStreamWriter<Row> dataStreamWriter = spark.readStream()
              .format("csv")
              .option("header", "true")
              .schema("<csv-schema>")
              .load("<csv-file-name>")
              // manipulate your streaming data
              .writeStream()
              .format("mongodb")
              .option("checkpointLocation", "/tmp/")
              .option("forceDeleteTempCheckpointLocation", "true")
              .option("spark.mongodb.connection.uri", "<mongodb-connection-string>")
              .option("spark.mongodb.database", "<database-name>")
              .option("spark.mongodb.collection", "<collection-name>")
              .outputMode("append");
             
            // run the query
            StreamingQuery query = dataStreamWriter.start();

     - id: python
       content: |

         1. Create a 
            `DataStreamReader <https://spark.apache.org/docs/latest/api/python/reference/pyspark.ss/api/pyspark.sql.streaming.DataStreamReader.html>`__ 
            that reads from the CSV file.

         #. Create a 
            `DataStreamWriter <https://spark.apache.org/docs/latest/api/python/reference/pyspark.ss/api/pyspark.sql.streaming.DataStreamWriter.html>`__
            by calling the ``writeStream()`` method on the streaming 
            DataFrame that you created with a ``DataStreamReader``. 
            Specify the format ``mongodb`` using the ``format()`` method.

         #. Call the ``start()`` method on the ``DataStreamWriter``
            instance to begin the stream.
         
         As the connector reads data from the CSV file, it adds that 
         data to MongoDB using the `outputMode <https://spark.apache.org/docs/latest/api/python/reference/pyspark.ss/api/pyspark.sql.streaming.DataStreamWriter.outputMode.html#pyspark.sql.streaming.DataStreamWriter.outputMode>`__ 
         you specify.

         .. code-block:: python
            :copyable: true

            # create a local SparkSession
            spark = SparkSession.builder \
              .appName("writeExample") \
              .master("spark://spark-master:<port>") \
              .config("spark.jars", "<mongo-spark-connector-JAR-file-name>") \
              .getOrCreate()

            # define a streaming query
            dataStreamWriter = (spark.readStream
              .format("csv")
              .option("header", "true")
              .schema(<csv-schema>)
              .load(<csv-file-name>)
              # manipulate your streaming data
              .writeStream
              .format("mongodb")
              .option("checkpointLocation", "/tmp/pyspark/")
              .option("forceDeleteTempCheckpointLocation", "true")
              .option("spark.mongodb.connection.uri", <mongodb-connection-string>)
              .option("spark.mongodb.database", <database-name>)
              .option("spark.mongodb.collection", <collection-name>)
              .outputMode("append")
            )

            # run the query
            query = dataStreamWriter.start()

     - id: scala
       content: |

         1. Create a 
            `DataStreamReader <https://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/streaming/DataStreamReader.html>`__ 
            that reads from the CSV file.

         #. Create a 
            `DataStreamWriter <https://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/streaming/DataStreamWriter.html>`__
            by calling the ``writeStream()`` method on the streaming 
            Dataset or streaming DataFrame that you created with a 
            ``DataStreamReader``. Specify the format ``mongodb`` using 
            the ``format()`` method.

         #. Call the ``start()`` method on the ``DataStreamWriter``
            instance to begin the stream.
         
         As the connector reads data from the CSV file, it adds that 
         data to MongoDB using the `outputMode <https://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/streaming/DataStreamWriter.html#outputMode(outputMode:String):org.apache.spark.sql.streaming.DataStreamWriter[T]>`__ 
         you specify.

         .. code-block:: scala
            :copyable: true

            // create a local SparkSession
            val spark = SparkSession.builder
              .appName("writeExample")
              .master("spark://spark-master:<port>")
              .config("spark.jars", "<mongo-spark-connector-JAR-file-name>")
              .getOrCreate()

            // define a streaming query
            val dataStreamWriter = spark.readStream
              .format("csv")
              .option("header", "true")
              .schema(<csv-schema>)
              .load(<csv-file-name>)
              // manipulate your streaming data
              .writeStream
              .format("mongodb")
              .option("checkpointLocation", "/tmp/")
              .option("forceDeleteTempCheckpointLocation", "true")
              .option("spark.mongodb.connection.uri", <mongodb-connection-string>)
              .option("spark.mongodb.database", <database-name>)
              .option("spark.mongodb.collection", <collection-name>)
              .outputMode("append")

            // run the query
            val query = dataStreamWriter.start()

DataStreamWriter API Documentation
----------------------------------

- `PySpark <https://spark.apache.org/docs/latest/api/python/reference/pyspark.ss/api/pyspark.sql.streaming.DataStreamWriter.html#pyspark.sql.streaming.DataStreamWriter>`__
- `Java <https://spark.apache.org/docs/latest/api/java/org/apache/spark/sql/streaming/DataStreamWriter.html>`__
- `Scala <https://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/streaming/DataStreamWriter.html>`__