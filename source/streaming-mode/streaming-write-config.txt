.. _spark-write-conf:

===========================
Write Configuration Options
===========================

.. contents:: On this page
   :local:
   :backlinks: none
   :depth: 1
   :class: singlecol

.. _spark-output-conf:

Write Configuration
-------------------

The following options for writing to MongoDB are available:

.. note::

   If you use ``SparkConf`` to set the connector's write configurations,
   prefix ``spark.mongodb.write.`` to each property.

.. list-table::
   :header-rows: 1
   :widths: 35 65

   * - Property name
     - Description
   
   * - ``connection.uri``
     - | **Required.**
       | The connection string configuration key.
       |
       | **Default:** ``mongodb://localhost:27017/``

   * - ``database``
     - | **Required.**
       | The database name configuration.

   * - ``collection``
     - | **Required.**
       | The collection name configuration.

   * - ``comment``
     - | The comment to append to the write operation. Comments appear in the 
         :manual:`output of the Database Profiler. </reference/database-profiler>`
       |
       | **Default:** None 

   * - ``convertJson``
     - | Specifies whether the connector parses the string and converts extended JSON
         into BSON.
       |
       | This setting accepts the following values:
       
       - ``any``: The connector converts all JSON values to BSON.

         - ``"{a: 1}"`` becomes ``{a: 1}``.
         - ``"[1, 2, 3]"`` becomes ``[1, 2, 3]``.
         - ``"true"`` becomes ``true``.
         - ``"01234"`` becomes ``1234``.
         - ``"{a:b:c}"`` doesn't change.

       - ``objectOrArrayOnly``: The connector converts only JSON objects and arrays to
         BSON.

         - ``"{a: 1}"`` becomes ``{a: 1}``. 
         - ``"[1, 2, 3]"`` becomes ``[1, 2, 3]``. 
         - ``"true"`` doesn't change.
         - ``"01234"`` doesn't change.
         - ``"{a:b:c}"`` doesn't change.

       - ``false``: The connector leaves all values as strings.

       | **Default:** ``false``
   
   * - ``idFieldList``
     - | Field or list of fields by which to split the collection data. To
         specify more than one field, separate them using a comma as shown
         in the following example:

       .. code-block:: none
          :copyable: false

          "fieldName1,fieldName2"

       | **Default:** ``_id``

   * - ``ignoreNullValues``
     - | When ``true``, the connector ignores any ``null`` values when writing,
         including ``null`` values in arrays and nested documents.
       |
       | **Default:** ``false``

   * - ``maxBatchSize``
     - | Specifies the maximum number of operations to batch in bulk
         operations.
       |
       | **Default:** ``512``

   * - ``mongoClientFactory``
     - | MongoClientFactory configuration key.
       | You can specify a custom implementation which must implement the
         ``com.mongodb.spark.sql.connector.connection.MongoClientFactory``
         interface.
       |
       | **Default:** ``com.mongodb.spark.sql.connector.connection.DefaultMongoClientFactory``

   * - ``operationType``
     - | Specifies the type of write operation to perform. You can set
         this to one of the following values:

       - ``insert``: Insert the data.
       - ``replace``: Replace an existing document that matches the
         ``idFieldList`` value with the new data. If no match exists, the
         value of ``upsertDocument`` indicates whether the connector
         inserts a new document.
       - ``update``: Update an existing document that matches the
         ``idFieldList`` value with the new data. If no match exists, the
         value of ``upsertDocument`` indicates whether the connector
         inserts a new document.

       |
       | **Default:** ``replace``

   * - ``ordered``
     - | Specifies whether to perform ordered bulk operations.
       |
       | **Default:** ``true``

   * - ``upsertDocument``
     - | When ``true``, replace and update operations will insert the data
         if no match exists.
       |
       | For time series collections, you must set ``upsertDocument`` to
         ``false``.
       |
       | **Default:** ``true``

   * - ``writeConcern.journal``
     - | Specifies ``j``, a write-concern option to enable request for
         acknowledgment that the data is confirmed on on-disk journal for
         the criteria specified in the ``w`` option. You can specify
         either ``true`` or ``false``.
       |
       | For more information on ``j`` values, see the MongoDB server
         guide on the
         :manual:`WriteConcern j option </reference/write-concern/#j-option>`.

   * - ``writeConcern.w``
     - | Specifies ``w``, a write-concern option to request acknowledgment
         that the write operation has propagated to a specified number of
         MongoDB nodes. For a list
         of allowed values for this option, see :manual:`WriteConcern
         </reference/write-concern/#w-option>` in the MongoDB manual.
       |
       | **Default:** ``1``

   * - ``writeConcern.wTimeoutMS``
     - | Specifies ``wTimeoutMS``, a write-concern option to return an error
         when a write operation exceeds the number of milliseconds. If you
         use this optional setting, you must specify a nonnegative integer.
       |
       | For more information on ``wTimeoutMS`` values, see the MongoDB server
         guide on the
         :manual:`WriteConcern wtimeout option </reference/write-concern/#wtimeout>`.

.. _configure-output-uri:

``connection.uri`` Configuration Setting
----------------------------------------

You can set all :ref:`spark-output-conf` via the write ``connection.uri``.

.. note::

   If you use ``SparkConf`` to set the connector's write configurations,
   prefix ``spark.mongodb.write.`` to the setting.

.. code:: cfg

  spark.mongodb.write.connection.uri=mongodb://127.0.0.1/test.myCollection

The configuration corresponds to the following separate configuration
settings:

.. code:: cfg

  spark.mongodb.write.connection.uri=mongodb://127.0.0.1/
   spark.mongodb.write.database=test
   spark.mongodb.write.collection=myCollection

If you specify a setting both in the ``connection.uri`` and in a separate
configuration, the ``connection.uri`` setting overrides the separate
setting. For example, in the following configuration, the
database for the connection is ``foobar``:

.. code:: cfg

  spark.mongodb.write.connection.uri=mongodb://127.0.0.1/foobar
   spark.mongodb.write.database=bar

.. _spark-change-stream-conf:

Change Streams
--------------

.. note::

   If you use ``SparkConf`` to set the connector's change stream
   configurations, prefix ``spark.mongodb.`` to each property.

.. list-table::
   :header-rows: 1
   :widths: 35 65

   * - Property name
     - Description

   * - ``change.stream.lookup.full.document``

     - Determines what values your change stream returns on update
       operations.

       The default setting returns the differences between the original
       document and the updated document.

       The ``updateLookup`` setting returns the differences between the
       original document and updated document as well as a copy of the
       entire updated document.

       **Default:** "default"

       .. tip::

          For more information on how this change stream option works,
          see the MongoDB server manual guide
          :manual:`Lookup Full Document for Update Operation </changeStreams/#lookup-full-document-for-update-operations>`.

   * - ``change.stream.micro.batch.max.partition.count``
     - | The maximum number of partitions the {+connector-short+} divides each 
         micro-batch into. Spark workers can process these partitions in parallel.
       |  
       | This setting applies only when using micro-batch streams.
       |
       | **Default**: ``1``

       .. warning:: Event Order

          Specifying a value larger than ``1`` can alter the order in which
          the {+connector-short+} processes change events. Avoid this setting
          if out-of-order processing could create data inconsistencies downstream. 

   * - ``change.stream.publish.full.document.only``
     - | Specifies whether to publish the changed document or the full
         change stream document.
       |
       | When this setting is ``true``, the connector exhibits the following behavior:
       
       - The connector filters out messages that
         omit the ``fullDocument`` field and only publishes the value of the
         field.
       - If you don't specify a schema, the connector infers the schema
         from the change stream document rather than from the underlying collection.

         **Default**: ``false``
       
       .. note::

          This setting overrides the ``change.stream.lookup.full.document``
          setting.

   * - ``change.stream.startup.mode``
     - | Specifies how the connector starts up when no offset is available.
       
       | This setting accepts the following values:
        
       - ``latest``: The connector begins processing
         change events starting with the most recent event.
         It will not process any earlier unprocessed events.
       - ``timestamp``: The connector begins processing change events at a specified time.
           
         To use the ``timestamp`` option, you must specify a time by using the
         ``change.stream.startup.mode.timestamp.start.at.operation.time`` setting.
         This setting accepts timestamps in the following formats:
         
         - An integer representing the number of seconds since the
           :wikipedia:`Unix epoch <Unix_time>`
         - A date and time in
           `ISO-8601 <https://www.iso.org/iso-8601-date-and-time-format.html>`__
           format with one-second precision
         - An extended JSON ``BsonTimestamp``
       
         **Default**: ``latest``
