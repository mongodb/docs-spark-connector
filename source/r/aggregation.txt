.. _r-aggregation:

===========
Aggregation
===========

.. default-domain:: mongodb

.. contents:: On this page
   :local:
   :backlinks: none
   :depth: 1
   :class: singlecol

Use MongoDB's :manual:`aggregation pipeline
</core/aggregation-pipeline/>` to apply filtering rules and perform
aggregation operations when reading data from MongoDB into Spark.

.. include:: /includes/example-load-dataframe.rst

Add a ``pipeline`` argument to ``read.df()`` from
within the ``sparkR`` shell to specify an aggregation pipeline
to use when creating a DataFrame.

.. code-block:: none

   agg_pipeline <- "{'$match': {'type': 'apple'}}"
   df <- read.df("", source = "com.mongodb.spark.sql.DefaultSource", pipeline = agg_pipeline)
   head(df)

.. include:: /includes/data-source.rst

In the ``sparkR`` shell, the operation prints the following output:

.. code-block:: none

     _id qty  type
   1   1   5 apple