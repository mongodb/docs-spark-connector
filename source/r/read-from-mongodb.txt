=================
Read from MongoDB
=================

.. default-domain:: mongodb

.. contents:: On this page
   :local:
   :backlinks: none
   :depth: 1
   :class: singlecol

You can create a Spark DataFrame to hold data from the MongoDB
collection specified in the
:ref:`spark.mongodb.input.uri<sparkR-shell>` option which your
``SparkSession`` option is using.

.. include:: /includes/example-load-dataframe.rst

Load the collection into a DataFrame with ``read.df()``
from within the ``sparkR`` shell.

.. code-block:: none

   df <- read.df("", source = "com.mongodb.spark.sql.DefaultSource")

.. include:: /includes/data-source.rst

Spark samples the records to infer the schema of the collection.
The following operation prints the schema to the console:

.. code-block:: none

   printSchema(df)

The operation produces the following shell output:

.. code-block:: none

   root
    |-- _id: double (nullable = true)
    |-- qty: double (nullable = true)
    |-- type: string (nullable = true)

Reading with Options
--------------------

You can add arguments to the ``read.df()`` method to specify
a MongoDB database and collection. The following example reads
from a collection called ``contacts`` in a database called
``people``.

.. code-block:: none

   df <- read.df("", source = "com.mongodb.spark.sql.DefaultSource",
                 database = "people", collection = "contacts")
