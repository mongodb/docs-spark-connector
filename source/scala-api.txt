===========================
Spark Connector Scala Guide
===========================

.. default-domain:: mongodb

.. admonition:: Source Code

   For the source code that contains the examples below, see
   :mongo-spark:`Introduction.scala
   </blob/master/examples/src/test/scala/tour/Introduction.scala>`.

.. _gs-prereq:

Prerequisites
-------------

.. include:: /includes/list-prerequisites.rst

.. _scala-getting-started:

Getting Started
---------------

Spark Shell
~~~~~~~~~~~

When starting the Spark shell, specify:

.. include:: /includes/extracts/command-line-start-spark-shell.rst

Import the MongoDB Connector Package
````````````````````````````````````

Enable MongoDB Connector specific functions and implicits for the
``SparkSession`` and RDD (Resilient Distributed Dataset) by importing
the following package in the Spark shell:

.. code-block:: scala

   import com.mongodb.spark._

Connect to MongoDB
``````````````````

Connection to MongoDB happens automatically when an RDD action
requires a :ref:`read <scala-read>` from MongoDB or a
:ref:`write <scala-write>` to MongoDB.

.. _scala-app:

Self-Contained Scala Application
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Dependency Management
`````````````````````

.. include:: /includes/scala-java-dependencies.rst

The following excerpt demonstrates how to include these dependencies in
a `SBT <http://www.scala-sbt.org/documentation.html>`_ ``build.scala`` file:

.. code-block:: scala

   scalaVersion := "2.11.7",
   libraryDependencies ++= Seq(
     "org.mongodb.spark" %% "mongo-spark-connector" % "{+current-version+}",
     "org.apache.spark" %% "spark-core" % "{+spark-core-version+}",
     "org.apache.spark" %% "spark-sql" % "{+spark-sql-version+}"
   )

Configuration
`````````````

.. include:: /includes/scala-java-sparksession-config.rst

.. code-block:: scala

   package com.mongodb

   object GettingStarted {

     def main(args: Array[String]): Unit = {
     
       /* Create the SparkSession.
        * If config arguments are passed from the command line using --conf,
        * parse args for the values to set.
        */
       import org.apache.spark.sql.SparkSession
     
       val spark = SparkSession.builder()
         .master("local")
         .appName("MongoSparkConnectorIntro")
         .config("spark.mongodb.input.uri", "mongodb://127.0.0.1/test.myCollection")
         .config("spark.mongodb.output.uri", "mongodb://127.0.0.1/test.myCollection")
         .getOrCreate()
         
     }
   }

MongoSpark Helper
-----------------

If you require granular control over your configuration, then the
``MongoSpark`` companion provides a ``builder()`` method for
configuring all aspects of the Mongo Spark Connector. It also provides
methods to create an RDD, ``DataFrame`` or ``Dataset``.

Troubleshooting
---------------

If you get a ``java.net.BindException: Can't assign requested address``,

- Check to ensure that you do not have another Spark shell already
  running.

- Try setting the ``SPARK_LOCAL_IP`` environment variable; e.g.

  .. code-block:: sh
  
     export SPARK_LOCAL_IP=127.0.0.1

- Try including the following option when starting the Spark shell:

  .. code-block:: sh

     --driver-java-options "-Djava.net.preferIPv4Stack=true"

If you have errors running the examples in this tutorial, you may need
to clear your local ivy cache (``~/.ivy2/cache/org.mongodb.spark`` and
``~/.ivy2/jars``).

Tutorials
---------

- :doc:`/scala/write-to-mongodb`
- :doc:`/scala/read-from-mongodb`
- :doc:`/scala/aggregation`
- :doc:`/scala/datasets-and-sql`
- :doc:`/scala/streaming`

.. class:: hidden

   .. toctree::
      :titlesonly:
   
      /scala/write-to-mongodb
      /scala/read-from-mongodb
      /scala/aggregation
      /scala/datasets-and-sql
      /scala/streaming
