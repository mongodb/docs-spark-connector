To write data to MongoDB, call the ``write()`` method on your
DataFrame object. This method returns a
`DataFrameWriter <https://spark.apache.org/docs/latest/api/java>`__
object, which you can use to specify the format and other configuration settings for your
batch write operation. 

You must specify the following configuration settings to write to MongoDB:
         
.. list-table::
   :header-rows: 1
   :stub-columns: 1
   :widths: 10 40
         
   * - Setting
     - Description
         
   * - ``dataFrame.write.format()``
     - Specifies the format of the underlying output data source. Use ``mongodb``
       to write to MongoDB.
         
   * - ``dataFrame.write.option()``
     - Use the ``option`` method to configure batch write settings, including the
       MongoDB deployment
       :manual:`connection string </reference/connection-string/>`,
       MongoDB database and collection, and
       destination directory.

       For a list of batch write configuration options, see
       the :ref:`spark-batch-write-conf` guide.

The following example creates a DataFrame from a ``json`` file and 
saves it to the MongoDB database and collection specified in ``SparkConf``:

.. code-block:: java

   Dataset<Row> dataFrame = spark.read().format("json")
                                        .load("example.json");

   dataFrame.write().format("mongodb")
                    .mode("overwrite")
                    .save();

.. include:: /includes/save-modes-tip.rst