The following example creates a DataFrame from a ``json`` file and 
saves it to the MongoDB collection specified in ``SparkConf``:

.. code-block:: java

   Dataset<Row> df = spark.read().format("json").load("example.json");

   df.write().format("mongodb").mode("overwrite").save();

The MongoDB Connector for Spark supports the following save modes:

-  ``append``
- ``overwrite``

To learn more about save modes, see the `Spark SQL Guide <https://spark.apache.org/docs/3.2.0/sql-data-sources-load-save-functions.html#save-modes>`__.
   