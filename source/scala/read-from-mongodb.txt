Use your local SparkSession's ``read`` method to create a DataFrame 
representing a collection.

.. note::
   
   A ``DataFrame`` is represented by a ``Dataset`` of
   ``Rows``. It is an alias of ``Dataset[Row]``.

The following example loads the collection specified in the
``SparkConf``:

.. code-block:: scala

   val df = spark.read.format("mongodb").load() // Uses the SparkConf for configuration

To specify a different collection, database, and other :ref:`read
configuration settings <spark-input-conf>`, use the ``option`` method:

.. code-block:: scala

   val df = spark.read.format("mongodb").option("database", "<example-database>").option("collection", "<example-collection>").load()
