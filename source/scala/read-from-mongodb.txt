.. _scala-read:

=================
Read From MongoDB
=================


Use the ``MongoSpark.load`` method to create an RDD representing
a collection.

The following example loads the collection specified in the
``SparkConf``:

.. code-block:: scala

   val rdd = MongoSpark.load(sc)
   
   println(rdd.count)
   println(rdd.first.toJson)

To specify a different collection, database, and other :ref:`read
configuration settings <spark-input-conf>`, pass a ``ReadConfig`` to
``MongoSpark.load()``.

.. _gs-read-config:

Using a ``ReadConfig``
``````````````````````

.. include:: /includes/scala-java-read-readconfig.rst

.. code-block:: scala

   import com.mongodb.spark.config._

   val readConfig = ReadConfig(Map("collection" -> "spark", "readPreference.name" -> "secondaryPreferred"), Some(ReadConfig(sc)))
   val customRdd = MongoSpark.load(sc, readConfig)
   
   println(customRdd.count)
   println(customRdd.first.toJson)

SparkContext Load Helper Methods
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

``SparkContext`` has an implicit helper method ``loadFromMongoDB()`` to
load data from MongoDB.

For example, use the ``loadFromMongoDB()`` method without any arguments
to load the collection specified in the ``SparkConf``:

.. code-block:: scala

   sc.loadFromMongoDB() // Uses the SparkConf for configuration

Call ``loadFromMongoDB()`` with a ``ReadConfig`` object to specify a
different MongoDB server address, database and collection. See
:ref:`input configuration settings <spark-output-conf>` for available
settings:

.. code-block:: scala

   sc.loadFromMongoDB(ReadConfig(Map("uri" -> "mongodb://example.com/database.collection"))) // Uses the ReadConfig
