Getting Started
---------------

This tutorial works either as a self-contained Scala application or as
individual commands in the Spark Shell.

Insert the following documents to the ``characters`` collection:

.. code-block:: scala
   
   package com.mongodb

   object SparkSQL {

     def main(args: Array[String]): Unit = {
    
       import org.apache.spark.sql.SparkSession
    
       /* For Self-Contained Scala Apps: Create the SparkSession
        * CREATED AUTOMATICALLY IN spark-shell */
       val sparkSession = SparkSession.builder()
         .master("local")
         .appName("MongoSparkConnectorIntro")
         .config("spark.mongodb.read.uri", "mongodb://127.0.0.1/test.characters")
         .config("spark.mongodb.write.uri", "mongodb://127.0.0.1/test.characters")
         .getOrCreate()

       import com.mongodb.spark._
       import com.mongodb.spark.config._
       import org.bson.Document

       val docs = """
         {"name": "Bilbo Baggins", "age": 50}
         {"name": "Gandalf", "age": 1000}
         {"name": "Thorin", "age": 195}
         {"name": "Balin", "age": 178}
         {"name": "Kíli", "age": 77}
         {"name": "Dwalin", "age": 169}
         {"name": "Óin", "age": 167}
         {"name": "Glóin", "age": 158}
         {"name": "Fíli", "age": 82}
         {"name": "Bombur"}""".trim.stripMargin.split("[\\r\\n]+").toSeq
       sparkSession.sparkContext.parallelize(docs.map(Document.parse)).saveToMongoDB()
       
       // Additional operations go here...

       }
   }

DataFrames and Datasets
-----------------------

A ``DataFrame`` is represented by a ``Dataset`` of
``Rows``. It is an alias of ``Dataset[Row]``.

The Mongo Spark Connector provides the
``com.mongodb.spark.sql.connector.MongoTableProvider`` class that 
creates ``DataFrames`` and ``Datasets`` from MongoDB.

.. code-block:: scala

   val df = MongoSpark.load(spark)  // Uses the SparkSession
   df.printSchema()                        // Prints DataFrame schema

The operation prints the following:

.. code-block:: none

   root
    |-- _id: struct (nullable = true)
    |    |-- oid: string (nullable = true)
    |-- age: integer (nullable = true)
    |-- name: string (nullable = true)

.. note::

   By default, reading from MongoDB in a ``SparkSession`` infers the
   schema by sampling documents from the database. To explicitly
   declare a schema, see :ref:`sql-declare-schema`.

Alternatively, you can use ``SparkSession`` methods to create DataFrames:

.. code-block:: scala

   val df2 = sparkSession.read.format("mongodb").options(...).load()

Filters
-------

.. include:: /includes/pushed-filters.rst

The following example filters and output the characters with ages under
100:

.. code-block:: scala

   df.filter(df("age") < 100).show()

The operation outputs the following:

.. code-block:: none

   +--------------------+---+-------------+
   |                 _id|age|         name|
   +--------------------+---+-------------+
   |[5755d7b4566878c9...| 50|Bilbo Baggins|
   |[5755d7b4566878c9...| 82|         Fíli|
   |[5755d7b4566878c9...| 77|         Kíli|
   +--------------------+---+-------------+

.. _sql-declare-schema:

Explicitly Declare a Schema
---------------------------

.. |class| replace:: ``case class``

.. include:: /includes/scala-java-explicit-schema.rst

.. code-block:: scala
  
   case class Character(name: String, age: Int)

.. important::
   For self-contained Scala applications, the ``Character`` class
   should be defined outside of the method using the class.
   
.. code-block:: scala  

   val explicitDF = MongoSpark.load[Character](sparkSession)
   explicitDF.printSchema()

The operation prints the following output:

.. code-block:: none

   root
    |-- name: string (nullable = true)
    |-- age: integer (nullable = false)

Save DataFrames to MongoDB
--------------------------

The MongoDB Spark Connector provides the ability to persist DataFrames
to a collection in MongoDB.

The following example uses ``MongoSpark.save(DataFrameWriter)`` method
to save the ``centenarians`` into the ``hundredClub`` collection in
MongoDB and to verify the save, reads from the ``hundredClub``
collection:

.. code-block:: scala

   MongoSpark.save(centenarians.write.option("collection", "hundredClub").mode("overwrite"))

   println("Reading from the 'hundredClub' collection:")
   MongoSpark.load(sparkSession.option("collection", "hundredClub")).show()

The DataFrameWriter includes the ``.mode("overwrite")`` to drop the
``hundredClub`` collection before writing the results, if the
collection already exists.

In the Spark Shell, the operation prints the following output:

.. code-block:: none

   +-------+----+
   |   name| age|
   +-------+----+
   |Gandalf|1000|
   | Thorin| 195|
   |  Balin| 178|
   | Dwalin| 169|
   |    Óin| 167|
   |  Glóin| 158|
   +-------+----+

``MongoSpark.save(dataFrameWriter)`` is shorthand for configuring and
saving via the DataFrameWriter. The following examples write DataFrames
to MongoDB using the DataFrameWriter directly:

.. code-block:: scala

   centenarians.write.option("collection", "hundredClub").mode("overwrite").mongo()
   centenarians.write.option("collection", "hundredClub").mode("overwrite").format("mongodb").save()
