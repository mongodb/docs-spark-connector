Getting Started
---------------

This tutorial works either as a self-contained Scala application or as
individual commands in the Spark Shell.

Insert the following documents to the ``characters`` collection:

.. code-block:: scala
   
   package com.mongodb

   object SparkSQL {

     def main(args: Array[String]): Unit = {
    
       import org.apache.spark.sql.SparkSession
    
       /* For Self-Contained Scala Apps: Create the SparkSession
        * CREATED AUTOMATICALLY IN spark-shell */
       val sparkSession = SparkSession.builder()
         .master("local")
         .appName("MongoSparkConnectorIntro")
         .config("spark.mongodb.read.uri", "mongodb://127.0.0.1/test.characters")
         .config("spark.mongodb.write.uri", "mongodb://127.0.0.1/test.characters")
         .getOrCreate()

       import com.mongodb.spark._
       import com.mongodb.spark.config._
       import org.bson.Document

       val docs = """
         {"name": "Bilbo Baggins", "age": 50}
         {"name": "Gandalf", "age": 1000}
         {"name": "Thorin", "age": 195}
         {"name": "Balin", "age": 178}
         {"name": "Kíli", "age": 77}
         {"name": "Dwalin", "age": 169}
         {"name": "Óin", "age": 167}
         {"name": "Glóin", "age": 158}
         {"name": "Fíli", "age": 82}
         {"name": "Bombur"}""".trim.stripMargin.split("[\\r\\n]+").toSeq
       sparkSession.sparkContext.parallelize(docs.map(Document.parse)).saveToMongoDB()
       
       // Additional operations go here...

       }
   }

DataFrames and Datasets
-----------------------

The Mongo Spark Connector provides the
``com.mongodb.spark.sql.connector.MongoTableProvider`` class that 
creates ``DataFrames`` and ``Datasets`` from MongoDB.

.. code-block:: scala

   val df = MongoSpark.load(spark)  // Uses the SparkSession
   df.printSchema()                        // Prints DataFrame schema

The operation prints the following:

.. code-block:: none

   root
    |-- _id: struct (nullable = true)
    |    |-- oid: string (nullable = true)
    |-- age: integer (nullable = true)
    |-- name: string (nullable = true)

.. note::

   By default, reading from MongoDB in a ``SparkSession`` infers the
   schema by sampling documents from the database. To explicitly
   declare a schema, see :ref:`sql-declare-schema`.

Alternatively, you can use ``SparkSession`` methods to create DataFrames:

.. code-block:: scala

   val df2 = sparkSession.read.format("mongodb").options(...).load()
