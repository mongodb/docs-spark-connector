.. _scala-write:

================
Write to MongoDB
================

.. include:: /includes/scala-java-write.rst

.. code-block:: scala

   import org.bson.Document

   val documents = sc.parallelize((1 to 10).map(i => Document.parse(s"{test: $i}")))

   MongoSpark.save(documents) // Uses the SparkConf for configuration

.. _gs-write-config:

Using a ``WriteConfig``
-----------------------

.. include:: /includes/scala-java-write-writeconfig.rst

.. code-block:: scala

   import com.mongodb.spark.config._

   val writeConfig = WriteConfig(Map("collection" -> "spark", "writeConcern.w" -> "majority"), Some(WriteConfig(sc)))
   val sparkDocuments = sc.parallelize((1 to 10).map(i => Document.parse(s"{spark: $i}")))
   
   MongoSpark.save(sparkDocuments, writeConfig)

.. _rdd-save-methods:

RDD Save Helper Methods
-----------------------

RDDs have an implicit helper method ``saveToMongoDB()`` to write data
to MongoDB:

For example, the following uses the ``documents`` RDD defined above and
uses its ``saveToMongoDB()`` method without any arguments to save the
documents to the collection specified in the ``SparkConf``:

.. code-block:: scala

   documents.saveToMongoDB() // Uses the SparkConf for configuration

Call ``saveToMongoDB()`` with a ``WriteConfig`` object to specify a
different MongoDB server address, database and collection. See
:ref:`write configuration settings <spark-output-conf>` for available
settings:

.. code-block:: scala

   documents.saveToMongoDB(WriteConfig(Map("uri" -> "mongodb://example.com/database.collection")))
   // Uses the WriteConfig

Unsupported Types
-----------------

Some Scala types (e.g. ``Lists``) are unsupported and should be
converted to their Java equivalent. To convert from Scala into native
types include the following import statement to use the ``.asJava``
method:

The following operation imports the ``.asJava`` method, converts a
Scala list to its Java equivalent, and saves it to MongoDB:

.. code-block:: scala

   import scala.collection.JavaConverters._
   import org.bson.Document
   
   val documents = sc.parallelize(
     Seq(new Document("fruits", List("apples", "oranges", "pears").asJava))
   )
   MongoSpark.save(documents)