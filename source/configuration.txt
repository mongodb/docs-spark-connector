=====================
Configuration Options
=====================

.. default-domain:: mongodb

.. contents:: On this page
   :local:
   :backlinks: none
   :depth: 1
   :class: singlecol

Various configuration options are available for the MongoDB Spark
Connector. To learn more about the options you can set, see 
:ref:`spark-write-conf` and :ref:`spark-read-conf`.

Specify Configuration
---------------------

.. _spark-conf:

Using ``SparkConf``
~~~~~~~~~~~~~~~~~~~

You can specify configuration options with ``SparkConf`` using any of 
the following approaches:

.. todo consider expanding these approaches into subsections and providing examples for each

.. tabs-selector:: drivers

.. tabs-drivers::

   tabs:
     - id: java-sync
       content: |

         - the ``SparkConf`` constructor in your application. To learn more, see the `Java SparkConf documentation <https://spark.apache.org/docs/latest/api/java/index.html?org/apache/spark/SparkConf.html>`__.

     - id: python
       content: |

         - the ``SparkConf`` constructor in your application. To learn more, see the `Python SparkConf documentation <https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.SparkConf.html>`__.

     - id: scala
       content: |

         - the ``SparkConf`` constructor in your application. To learn more, see the `Scala SparkConf documentation <https://spark.apache.org/docs/latest/api/scala/org/apache/spark/SparkConf.html>`__.

- the ``--conf`` flag at runtime. To learn more, see 
  `Dynamically Loading Spark Properties <https://spark.apache.org/docs/latest/configuration.html#dynamically-loading-spark-properties>`__ in 
  the Spark documentation.

- the ``$SPARK_HOME/conf/spark-default.conf`` file.

The MongoDB Spark Connector will use the settings in ``SparkConf`` as 
defaults.

.. important::

   When setting configurations with ``SparkConf``, you must prefix the
   configuration options. Refer to :ref:`spark-write-conf` and 
   :ref:`spark-read-conf` for the specific prefixes.

.. _read-write-config:

Using ``ReadConfig`` and ``WriteConfig``
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Various methods in the MongoDB Connector API accept an optional
``ReadConfig`` or a ``WriteConfig`` object. These settings override any 
corresponding settings in ``SparkConf``.

For examples, see :ref:`gs-read-config` and :ref:`gs-write-config`.

.. todo consider moving above linked examples onto this page - they are within tabs, so the links won't be as helpful

To learn more about :mongo-spark:`ReadConfig
</blob/master/src/main/scala/com/mongodb/spark/config/ReadConfig.scala>`
or :mongo-spark:`WriteConfig
</blob/master/src/main/scala/com/mongodb/spark/config/WriteConfig.scala>` objects, refer to the source for these methods.

.. _options-map:

Using an Options Map
~~~~~~~~~~~~~~~~~~~~

In the Spark API, the DataFrameReader and DataFrameWriter methods 
accept options in the form of a ``Map[String, String]``. Options 
specified this way override any corresponding settings in ``SparkConf``.

.. todo consider adding an example

.. tip::
   
   You can convert custom ``ReadConfig`` or ``WriteConfig`` settings 
   into a ``Map`` using the ``asOptions()`` method.

.. todo create example of .asOptions() above

.. tabs-drivers::

   tabs:
     - id: java-sync
       content: |

         To learn more about specifying options with 
         `DataFrameReader <https://spark.apache.org/docs/latest/api/java/org/apache/spark/sql/DataFrameReader.html#option-java.lang.String-boolean->`__ and 
         `DataFrameWriter <https://spark.apache.org/docs/latest/api/java/org/apache/spark/sql/DataFrameWriter.html#option-java.lang.String-boolean->`__, 
         refer to the Java Spark documentation for the ``.option()`` 
         method.

     - id: python
       content: |

         To learn more about specifying options with 
         `DataFrameReader <https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrameReader.option.html#pyspark.sql.DataFrameReader.option>`__ and 
         `DataFrameWriter <https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrameWriter.option.html#pyspark.sql.DataFrameWriter.option>`__, 
         refer to the Java Spark documentation for the ``.option()`` 
         method.

     - id: scala
       content: |

         To learn more about specifying options with 
         `DataFrameReader <https://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/DataFrameReader.html#option(key:String,value:Double):org.apache.spark.sql.DataFrameReader>`__ and 
         `DataFrameWriter <https://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/DataFrameWriter.html#option(key:String,value:Double):org.apache.spark.sql.DataFrameWriter[T]>`__, 
         refer to the Java Spark documentation for the ``.option()`` 
         method.
  
Short-Form Syntax
`````````````````

Options maps support short-form syntax. You may omit the prefix when 
specifying an option key string.

.. example::

   The following syntaxes are equivalent to one another:

   - ``dfw.option("spark.mongodb.collection", "myCollection").save()``

   - ``dfw.option("collection", "myCollection").save()``

Using a System Property
~~~~~~~~~~~~~~~~~~~~~~~

The connector provides a cache for ``MongoClients`` which can only be
configured with a System Property. See :ref:`cache-configuration`.

.. _cache-configuration:

Cache Configuration
-------------------

The MongoConnector includes a cache for MongoClients, so workers can
share the MongoClient across threads.

.. important::

   As the cache is setup before the Spark Configuration is available,
   the cache can only be configured with a System Property.

.. list-table::
   :header-rows: 1
   :widths: 35 65

   * - System Property name
     - Description

   * - ``mongodb.keep_alive_ms``
     - The length of time to keep a ``MongoClient`` available for 
       sharing.

       **Default:** ``5000``

``ConfigException``\s
---------------------

A configuration error throws a ``ConfigException``. Confirm that any of 
the following methods of configuration that you use are configured 
properly:

- :ref:`SparkConf <spark-conf>`
- :ref:`ReadConfig and WriteConfig <read-write-config>` objects
- :ref:`Options maps <options-map>`

.. toctree::
   :titlesonly:

   configuration/write
   configuration/read
