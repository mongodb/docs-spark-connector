.. _spark-structured-streaming:

=================================
Structured Streaming with MongoDB
=================================

.. default-domain:: mongodb

.. contents:: On this page
   :local:
   :backlinks: none
   :depth: 2
   :class: singlecol

Overview
--------

Spark Structured Streaming is a stream processing engine you can use 
through the Dataset or DataFrame API.

.. include:: includes/streaming-distinction.rst

You can use the MongoDB Spark Connector with Spark Structured Streaming 
to apply filters, aggregation pipelines, and other MongoDB features to 
your streaming data.

To learn more about Structured Streaming, see the 
`Spark Programming Guide
<https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html>`__.
   
.. _read-structured-stream:

Configuring a Read Stream
-------------------------

.. note:: 
   
   For general configuration settings for the MongoDB Spark Connector, 
   see :ref:`spark-input-conf` and :ref:`spark-output-conf`.

Specify ``.readStream`` configuration settings on your local 
SparkSession.

.. tabs-drivers::

   tabs:
     - id: java-sync
       content: |

     - id: python
       content: |

         .. code-block:: python
            :copyable: true
         
            <local SparkSession> \
              .readStream \
              .format("csv") \
              .option("header", "true") \
              .schema(exampleSchema) \
              .load("example*.csv")
         
         For a complete list of methods, see the 
         `pyspark Structured Streaming reference <https://spark.apache.org/docs/latest/api/python/reference/pyspark.ss.html>`__.

     - id: scala
       content: |

.. _continuous-processing:

Continuous Processing
~~~~~~~~~~~~~~~~~~~~~

Reading a stream from a MongoDB database requires 
*continuous processing*, 
an experimental feature introduced in Spark version 2.3. To learn 
more about continuous processing, see the `Spark documentation <https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#continuous-processing>`__.

To use continuous processing with the MongoDB Spark Connector, add the 
``.trigger()`` method to the ``.writeStream`` of the streaming 
Dataset or DataFrame that you create from your MongoDB ``.readStream``.

.. note:: 

   Reading a stream from MongoDB with ``.readStream`` depends 
   upon watching :manual:`change streams </changeStreams>` for 
   real-time updates.

.. tabs-drivers::

   tabs:
     - id: java-sync
       content: |

     - id: python
       content: |

         .. code-block:: python
            :copyable: true
         
            streamingDataFrame = (<local SparkSession>
              .readStream
              .format("mongodb")
              .schema(exampleSchema)
              .load()
            )
         
            streamingDataFrame \
              .writeStream \
              .trigger(continuous="1 second") \
              .start()

     - id: scala
       content: |

.. _write-structured-stream:

Configuring a Write Stream
--------------------------

.. note:: 
   
   For general configuration settings for the MongoDB Spark Connector, 
   see :ref:`spark-input-conf` and :ref:`spark-output-conf`.

Specify ``.writeStream`` configuration settings on your streaming 
Dataset or DataFrame.

.. tabs-drivers::

   tabs:
     - id: java-sync
       content: |

     - id: python
       content: |

         .. code-block:: python
            :copyable: true
         
            <streaming Dataset/ DataFrame> \
              .writeStream \
              .format("mongodb") \
              .queryName("exampleQuery") \
              .option("checkpointLocation", "/tmp/checkpoint/") \
              .option("forceDeleteTempCheckpointLocation", "true") \
              .option('spark.mongodb.connection.uri', 'mongodb://mongo1,mongo2,mongo3/exampleDatabase.exampleCollection?replicaSet=rs0') \
              .outputMode("complete")

         For a complete list of methods, see the 
         `pyspark Structured Streaming reference <https://spark.apache.org/docs/latest/api/python/reference/pyspark.ss.html>`__.

     - id: scala
       content: |

Usage Examples
--------------

Prerequisites
~~~~~~~~~~~~~

.. tabs-selector:: drivers

.. tabs-drivers::

   tabs:
     - id: java-sync
       content: |

     - id: python
       content: |

         The following examples use a Docker environment that provides 
         a `JupyterLabs <https://jupyter.org/>`__ notebook running 
         `pyspark <https://spark.apache.org/docs/latest/api/python/>`__ 
         and a MongoDB replica set. You can download and run this 
         environment from this `MongoDB University GitHub repository <https://github.com/mongodb-university/mongodb-spark>`__.
  
         Ensure that you have access to the following files in your 
         environment:
         
         - The MongoDB Spark Connector version {+current-version+} 
           ``.jar`` file.
         
         - The ``daily_csv.csv`` file from 
           `DataHub.io <https://datahub.io/core/natural-gas#resource-daily>`__ 
           to use as sample data.
         
         .. tip:: Adding files to the example Docker environment
         
            Add each file you want to copy into your Docker 
            environment to your local
            `MongoDB University GitHub repository <https://github.com/mongodb-university/mongodb-spark>`__ 
            parent directory.
            
            Add the following line to the ``run`` script
            immediately after ``docker-compose exec`` commands:
         
            .. code-block:: sh
               :copyable: true
            
               docker cp <file-to-copy> jupyterlab:/home/jovyan/
         
            On Linux or MacOS, edit ``run.sh``.
         
            On Windows, edit ``run.ps1``.

     - id: scala
       content: |

Create a SparkSession
~~~~~~~~~~~~~~~~~~~~~

Create a ``SparkSession`` configured with MongoDB Spark Connector 
version {+current-version+} to use for the following examples.

.. tabs-drivers::

   tabs:
     - id: java-sync
       content: |

     - id: python
       content: |

         .. code-block:: python
            :copyable: true
         
            spark = (SparkSession
              .builder
              .master("spark://spark-master:7077")
              .config("spark.jars","mongosparkv10.jar")
              .getOrCreate()
            )

     - id: scala
       content: |

Stream to MongoDB from a CSV File
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. tabs-drivers::

   tabs:
     - id: java-sync
       content: |

         .. code-block:: java
            :copyable: true

     - id: python
       content: |

         To create a :ref:`write stream <write-structured-stream>` to 
         MongoDB from a ``.csv`` file, first create a `DataStreamReader <https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.streaming.DataStreamReader.html>`__ 
         from the ``.csv`` file, then use that DataStreamReader to 
         create a `DataStreamWriter <https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.streaming.DataStreamWriter.html>`__ 
         to MongoDB. Finally, ``.start()`` the stream.
         
         As streaming data is read from the ``.csv`` file, it is added 
         to MongoDB in the `outputMode <https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.streaming.DataStreamWriter.outputMode.html#pyspark.sql.streaming.DataStreamWriter.outputMode>`__ 
         you specify.

         .. code-block:: python
            :copyable: true
            :emphasize-lines: 9, 14, 21
 
            # define the schema of the .csv
            readSchema = (StructType()
              .add('Type', StringType())
              .add('Date', TimestampType())
              .add('Price', DoubleType())
            )

            query = (spark
              .readStream
              .format("csv")
              .option("header", "true")
              .schema(readSchema)
              .load("daily*.csv")
              .writeStream
              .format("mongodb")
              .option("checkpointLocation", "/tmp/pyspark/")
              .option("forceDeleteTempCheckpointLocation", "true")
              .option('spark.mongodb.connection.uri', 'mongodb://mongo1,mongo2,mongo3/Pricing.NaturalGas?replicaSet=rs0')
              .option('spark.mongodb.database', 'Pricing')
              .option('spark.mongodb.collection', 'NaturalGas')
              .outputMode("append")
            )

            # run the query
            sq = query.start()
            sq.processAllAvailable()
            sq.stop()
            
     - id: scala
       content: |

         .. code-block:: scala
            :copyable: true


Stream to a CSV File from MongoDB
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. tip:: Streaming via change streams

   Streaming from MongoDB requires 
   :ref:`continuous processing <continuous-processing>` 
   and an incoming stream of data to write out.

   Once your stream is running, try adding data to MongoDB and 
   observing your output ``.csv``.

.. tabs-drivers::

   tabs:
     - id: java-sync
       content: |

         .. code-block:: java
            :copyable: true

     - id: python
       content: |

         To create a :ref:`read stream <read-structured-stream>` to a 
         ``.csv`` file from MongoDB, first create a `DataStreamReader <https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.streaming.DataStreamReader.html>`__ 
         from MongoDB, then use that DataStreamReader to 
         create a `DataStreamWriter <https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.streaming.DataStreamWriter.html>`__ 
         to a new ``.csv`` file. Finally, ``.start()`` the stream.
         
         As new data is inserted into MongoDB, MongoDB streams that 
         data out to a ``.csv`` file in the `outputMode <https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.streaming.DataStreamWriter.outputMode.html#pyspark.sql.streaming.DataStreamWriter.outputMode>`__ 
         you specify.

         .. code-block:: python
            :copyable: true
            :emphasize-lines: 10, 18, 21-22

            # define the schema of the Source collection
            readSchema = (StructType()
              .add('company_symbol', StringType())
              .add('company_name', StringType())
              .add('price', DoubleType())
              .add('tx_time', TimestampType())
            )            

            query = (spark
              .readStream
              .format("mongodb")
              .option('spark.mongodb.connection.uri', 'mongodb://mongo1,mongo2,mongo3/Stocks.Source?replicaSet=rs0')
              .option('spark.mongodb.database', 'Stocks')
              .option('spark.mongodb.collection', 'Source')
              .schema(readSchema)
              .load()
              # manipulate your streaming data
              .writeStream
              .format("csv")
              .option("path", "/output/")
              .trigger(continuous="1 second")
              .outputMode("append")
            )

            # run the query
            sq = query.start()   

     - id: scala
       content: |

         .. code-block:: scala
            :copyable: true
