.. _spark-structured-streaming:

=================================
Structured Streaming with MongoDB
=================================

.. default-domain:: mongodb

.. contents:: On this page
   :local:
   :backlinks: none
   :depth: 2
   :class: singlecol

Overview
--------

Spark Structured Streaming is a data stream processing engine you can 
use through the Dataset or DataFrame API. The MongoDB Spark Connector 
enables you to stream to and from MongoDB using Spark Structured 
Streaming.

.. include:: includes/streaming-distinction.rst

To learn more about Structured Streaming, see the 
`Spark Programming Guide
<https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html>`__.

.. _write-structured-stream:

Configuring a Write Stream to MongoDB
-------------------------------------

.. tabs-selector:: drivers

.. tabs-drivers::

   tabs:

     - id: java-sync
       content: |

         Specify write stream configuration settings on your streaming 
         Dataset or streaming DataFrame using `DataStreamWriter <https://spark.apache.org/docs/latest/api/java/org/apache/spark/sql/streaming/DataStreamWriter.html>`__. 
         You must specify the following configuration 
         settings to write to MongoDB:
         
         .. list-table::
            :header-rows: 1
            :stub-columns: 1
            :widths: 10 40
         
            * - Setting
              - Description
         
            * - ``writeStream.format()``
              - The format to use for write stream data. Use 
                ``mongodb``.
         
            * - ``writeStream.option()``
              - Use the ``option`` method to specify your MongoDB 
                deployment :manual:`connection string </reference/connection-string/>` with the 
                ``spark.mongodb.connection.uri`` option key.
         
                You must specify a database and collection, either as 
                part of your connection string or with additional 
                ``option`` methods using the following keys:
         
                - ``spark.mongodb.database``
                - ``spark.mongodb.collection``
         
            * - ``writeStream.outputMode()``
              - Specifies how data of a streaming DataFrame/Dataset is 
                written to a streaming sink. To view a list of all 
                supported output modes, see `the Java outputMode documentation <https://spark.apache.org/docs/latest/api/java/org/apache/spark/sql/streaming/DataStreamWriter.html#outputMode-java.lang.String->`__.
         
         The following code snippet shows how to use the preceding 
         configuration settings to stream data to MongoDB:

         .. code-block:: java
            :copyable: true
            :emphasize-lines: 2-3, 6
         
            <streaming Dataset/DataFrame>.writeStream()
              .format("mongodb")
              .option("spark.mongodb.connection.uri", <mongodb-connection-string>)
              .option("spark.mongodb.database", <database-name>)
              .option("spark.mongodb.collection", <collection-name>)
              .outputMode("append");

         For a complete list of methods, see the 
         `Java Structured Streaming reference <https://spark.apache.org/docs/latest/api/java/org/apache/spark/sql/streaming/DataStreamWriter.html>`__. 

     - id: python
       content: |

         Specify write stream configuration settings on your streaming 
         DataFrame using `DataStreamWriter <https://spark.apache.org/docs/latest/api/python/reference/pyspark.ss/api/pyspark.sql.streaming.DataStreamWriter.html>`__. 
         You must specify the following configuration settings to write 
         to MongoDB:
         
         .. list-table::
            :header-rows: 1
            :stub-columns: 1
            :widths: 10 40
         
            * - Setting
              - Description
         
            * - ``writeStream.format()``
              - The format to use for write stream data. Use 
                ``mongodb``.
         
            * - ``writeStream.option()``
              - Use the ``option`` method to specify your MongoDB 
                deployment :manual:`connection string </reference/connection-string/>` with the 
                ``spark.mongodb.connection.uri`` option key.
         
                You must specify a database and collection, either as 
                part of your connection string or with additional 
                ``option`` methods using the following keys:
         
                - ``spark.mongodb.database``
                - ``spark.mongodb.collection``
         
            * - ``writeStream.outputMode()``
              - Specifies how data of a streaming DataFrame is 
                written to a streaming sink. To view a list of all 
                supported output modes, see the `pyspark outputMode documentation <https://spark.apache.org/docs/latest/api/python/reference/pyspark.ss/api/pyspark.sql.streaming.DataStreamWriter.outputMode.html#pyspark.sql.streaming.DataStreamWriter.outputMode>`__.

         
         The following code snippet shows how to use the preceding 
         configuration settings to stream data to MongoDB:

         .. code-block:: python
            :copyable: true
            :emphasize-lines: 2-3, 6
         
            <streaming DataFrame>.writeStream \
              .format("mongodb") \
              .option("spark.mongodb.connection.uri", <mongodb-connection-string>) \
              .option("spark.mongodb.database", <database-name>) \
              .option("spark.mongodb.collection", <collection-name>) \
              .outputMode("append")

         For a complete list of methods, see the 
         `pyspark Structured Streaming reference <https://spark.apache.org/docs/latest/api/python/reference/pyspark.ss/api/pyspark.sql.streaming.DataStreamWriter.html#pyspark.sql.streaming.DataStreamWriter>`__.

     - id: scala
       content: |

         Specify write stream configuration settings on your streaming 
         Dataset or streaming DataFrame using `DataStreamWriter <https://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/streaming/DataStreamWriter.html>`__. You must specify the following configuration 
         settings to write to MongoDB:
         
         .. list-table::
            :header-rows: 1
            :stub-columns: 1
            :widths: 10 40
         
            * - Setting
              - Description
         
            * - ``writeStream.format()``
              - The format to use for write stream data. Use 
                ``mongodb``.
         
            * - ``writeStream.option()``
              - Use the ``option`` method to specify your MongoDB 
                deployment :manual:`connection string </reference/connection-string/>` with the 
                ``spark.mongodb.connection.uri`` option key.
         
                You must specify a database and collection, either as 
                part of your connection string or with additional 
                ``option`` methods using the following keys:
         
                - ``spark.mongodb.database``
                - ``spark.mongodb.collection``
         
            * - ``writeStream.outputMode()``
              - Specifies how data of a streaming DataFrame/Dataset is 
                written to a streaming sink. To view a list of all 
                supported output modes, see `the Scala outputMode documentation <https://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/streaming/DataStreamWriter.html#outputMode(outputMode:String):org.apache.spark.sql.streaming.DataStreamWriter[T]>`__.
         
         The following code snippet shows how to use the preceding 
         configuration settings to stream data to MongoDB:

         .. code-block:: scala
            :copyable: true
            :emphasize-lines: 2-3, 6
         
            <streaming Dataset/DataFrame>.writeStream
              .format("mongodb")
              .option("spark.mongodb.connection.uri", <mongodb-connection-string>)
              .option("spark.mongodb.database", <database-name>)
              .option("spark.mongodb.collection", <collection-name>)
              .outputMode("append")

         For a complete list of methods, see the 
         `Scala Structured Streaming reference <https://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/streaming/DataStreamWriter.html>`__.

.. _read-structured-stream:
.. _continuous-processing:

Configuring a Read Stream from MongoDB
--------------------------------------

Reading a stream from a MongoDB database requires 
*continuous processing*, 
an experimental feature introduced in Spark version 2.3. To learn 
more about continuous processing, see the `Spark documentation <https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#continuous-processing>`__.

.. tabs-drivers::

   tabs:

     - id: java-sync
       content: |

         To use continuous processing with the MongoDB Spark Connector, 
         call the ``trigger()`` method on the ``writeStream`` property 
         of the streaming Dataset or streaming DataFrame that you 
         create from your MongoDB read stream. Pass a setting returned 
         by ``Trigger.Continuous()`` to the the ``trigger()`` method.
         
         .. include:: /includes/fact-read-from-change-stream
         
         Specify read stream configuration settings on 
         `DataStreamReader <https://spark.apache.org/docs/latest/api/java/org/apache/spark/sql/streaming/DataStreamReader.html>`__. You must specify the following 
         configuration settings to read from MongoDB:
         
         .. list-table::
            :header-rows: 1
            :stub-columns: 1
            :widths: 10 40
         
            * - Setting
              - Description
         
            * - ``readStream.format()``
              - The format to use for read stream data. Use ``mongodb``.
         
            * - ``writeStream.trigger()``
              - The policy that indicates how often results should be 
                written to the streaming sink. To view a list of all 
                supported policies, see `the Java Trigger documentation <https://spark.apache.org/docs/latest/api/java/org/apache/spark/sql/streaming/Trigger.html>`__.

                When set, this method enables continuous processing 
                for your read stream. Pass the method a time value 
                created by the ``Trigger.Continuous()`` method.

                .. include:: /includes/note-trigger-method

         The following code snippet shows how to use the preceding 
         configuration settings to stream data from MongoDB:

         .. code-block:: java
            :copyable: true
            :emphasize-lines: 1, 4, 8, 12

            import org.apache.spark.sql.streaming.Trigger;
 
            Dataset<Row> streamingDataset = <local SparkSession>.readStream()
              .format("mongodb")
              .load();
 
            DataStreamWriter<Row> dataStreamWriter = streamingDataset.writeStream()
              .trigger(Trigger.Continuous("1 second"))
              .format("memory")
              .outputMode("append");
 
            StreamingQuery query = dataStreamWriter.start();

         .. note::
            
            Spark does not begin streaming until you call the 
            ``start()`` method on a streaming query.

         For a complete list of methods, see the 
         `Java Structured Streaming reference <https://spark.apache.org/docs/latest/api/java/org/apache/spark/sql/streaming/DataStreamReader.html>`__.

     - id: python
       content: |

         To use continuous processing with the MongoDB Spark Connector, 
         call the ``trigger()`` method on the ``writeStream`` property 
         of the streaming DataFrame that you create from 
         your MongoDB read stream. In your ``trigger()``, specify the 
         ``continuous`` parameter.
         
         .. include:: /includes/fact-read-from-change-stream
         
         Specify read stream configuration settings on 
         `DataStreamReader <https://spark.apache.org/docs/latest/api/python/reference/pyspark.ss/api/pyspark.sql.streaming.DataStreamReader.html>`__.
         You must specify the following configuration settings to read from MongoDB:
         
         .. list-table::
            :header-rows: 1
            :stub-columns: 1
            :widths: 10 40
         
            * - Setting
              - Description
         
            * - ``readStream.format()``
              - The format to use for read stream data. Use ``mongodb``.
         
            * - ``writeStream.trigger()``
              - The policy that indicates how often results should be 
                written to the streaming sink. To view a list of all 
                supported policies, see the `pyspark trigger documentation <https://spark.apache.org/docs/latest/api/python/reference/pyspark.ss/api/pyspark.sql.streaming.DataStreamWriter.trigger.html#pyspark.sql.streaming.DataStreamWriter.trigger>`__.

                When set, this method enables continuous processing 
                for your read stream. Pass the method a time value 
                using the ``continuous`` parameter.

                .. include:: /includes/note-trigger-method

         The following code snippet shows how to use the preceding 
         configuration settings to stream data from MongoDB:

         .. code-block:: python
            :copyable: true
            :emphasize-lines: 2, 8, 12
         
            streamingDataFrame = (<local SparkSession>.readStream
              .format("mongodb")
              .load()
            )
         
            dataStreamWriter = (streamingDataFrame.writeStream
              .trigger(continuous="1 second")
              .format("memory")
              .outputMode("append")
            )

            query = dataStreamWriter.start()

         .. note::
            
            Spark does not begin streaming until you call the 
            ``start()`` method on a streaming query.

         For a complete list of methods, see the 
         `pyspark Structured Streaming reference <https://spark.apache.org/docs/latest/api/python/reference/pyspark.ss/api/pyspark.sql.streaming.DataStreamReader.html>`__.

     - id: scala
       content: |

         To use continuous processing with the MongoDB Spark Connector, 
         call the ``trigger()`` method on the ``writeStream`` property 
         of the streaming Dataset or streaming DataFrame that you 
         create from your MongoDB read stream. Pass a setting returned 
         by ``Trigger.Continuous()`` to the the ``trigger()`` method.
         
         .. include:: /includes/fact-read-from-change-stream
         
         Specify read stream configuration settings on 
         `DataStreamReader <https://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/streaming/DataStreamReader.html>`__. You must specify the following 
         configuration settings to read from MongoDB:
         
         .. list-table::
            :header-rows: 1
            :stub-columns: 1
            :widths: 10 40
         
            * - Setting
              - Description
         
            * - ``readStream.format()``
              - The format to use for read stream data. Use ``mongodb``.
         
            * - ``writeStream.trigger()``
              - The policy that indicates how often results should be 
                written to the streaming sink. To view a list of all 
                supported policies, see `the Scala trigger documentation <https://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/streaming/DataStreamWriter.html#trigger(trigger:org.apache.spark.sql.streaming.Trigger):org.apache.spark.sql.streaming.DataStreamWriter[T]>`__.

                When set, this method enables continuous processing 
                for your read stream. Pass the method a time value 
                created by the ``Trigger.Continuous()`` method.

                .. include:: /includes/note-trigger-method

         The following code snippet shows how to use the preceding 
         configuration settings to stream data from MongoDB:

         .. code-block:: scala
            :copyable: true
            :emphasize-lines: 1, 4, 8, 12

            import org.apache.spark.sql.streaming.Trigger
         
            val streamingDataFrame = <local SparkSession>.readStream
              .format("mongodb")
              .load()
         
            val dataStreamWriter = streamingDataFrame.writeStream
              .trigger(Trigger.Continuous("1 second"))
              .format("memory")
              .outputMode("append")

            val query = dataStreamWriter.start()

         .. note::
            
            Spark does not begin streaming until you call the 
            ``start()`` method on a streaming query.

         For a complete list of methods, see the 
         `Scala Structured Streaming reference <https://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/streaming/DataStreamReader.html>`__.

Examples
--------

The following examples show Spark Structured Streaming configurations 
for streaming to and from MongoDB.

Stream to MongoDB from a CSV File
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

To stream data from a CSV file to MongoDB:

.. tabs-drivers::

   tabs:

     - id: java-sync
       content: |

         1. Create a 
            `DataStreamReader <https://spark.apache.org/docs/latest/api/java/org/apache/spark/sql/streaming/DataStreamReader.html>`__ 
            that reads from the CSV file.

         #. Create a 
            `DataStreamWriter <https://spark.apache.org/docs/latest/api/java/org/apache/spark/sql/streaming/DataStreamWriter.html>`__ 
            by calling the ``writeStream()`` method on the streaming 
            Dataset or streaming DataFrame that you created with a 
            ``DataStreamReader``. Specify the format ``mongodb`` using 
            the ``format()`` method.

         #. Call the ``start()`` method on the ``DataStreamWriter``
            instance to begin the stream.
         
         As the connector reads data from the CSV file, it adds that 
         data to MongoDB using the `outputMode <https://spark.apache.org/docs/latest/api/java/org/apache/spark/sql/streaming/DataStreamWriter.html#outputMode-java.lang.String->`__ 
         you specify.

         .. code-block:: java
            :copyable: true

            // create a local SparkSession
            SparkSession spark = SparkSession.builder()
              .appName("writeExample")
              .master("spark://spark-master:<port>")
              .config("spark.jars", "<mongo-spark-connector-JAR-file-name>")
              .getOrCreate();
             
            // define a streaming query
            DataStreamWriter<Row> dataStreamWriter = spark.readStream()
              .format("csv")
              .option("header", "true")
              .schema("<csv-schema>")
              .load("<csv-file-name>")
              // manipulate your streaming data
              .writeStream()
              .format("mongodb")
              .option("checkpointLocation", "/tmp/")
              .option("forceDeleteTempCheckpointLocation", "true")
              .option("spark.mongodb.connection.uri", "<mongodb-connection-string>")
              .option("spark.mongodb.database", "<database-name>")
              .option("spark.mongodb.collection", "<collection-name>")
              .outputMode("append");
             
            // run the query
            StreamingQuery query = dataStreamWriter.start();

     - id: python
       content: |

         1. Create a 
            `DataStreamReader <https://spark.apache.org/docs/latest/api/python/reference/pyspark.ss/api/pyspark.sql.streaming.DataStreamReader.html>`__
            that reads from the CSV file.

         #. Create a 
            `DataStreamWriter <https://spark.apache.org/docs/latest/api/python/reference/pyspark.ss/api/pyspark.sql.streaming.DataStreamWriter.html>`__
            by calling the ``writeStream()`` method on the streaming 
            DataFrame that you created with a ``DataStreamReader``. 
            Specify the format ``mongodb`` using the ``format()`` method.

         #. Call the ``start()`` method on the ``DataStreamWriter``
            instance to begin the stream.
         
         As the connector reads data from the CSV file, it adds that 
         data to MongoDB using the `outputMode <https://spark.apache.org/docs/latest/api/python/reference/pyspark.ss/api/pyspark.sql.streaming.DataStreamWriter.outputMode.html#pyspark.sql.streaming.DataStreamWriter.outputMode>`__
         you specify.

         .. code-block:: python
            :copyable: true

            # create a local SparkSession
            spark = SparkSession.builder \
              .appName("writeExample") \
              .master("spark://spark-master:<port>") \
              .config("spark.jars", "<mongo-spark-connector-JAR-file-name>") \
              .getOrCreate()

            # define a streaming query
            dataStreamWriter = (spark.readStream
              .format("csv")
              .option("header", "true")
              .schema(<csv-schema>)
              .load(<csv-file-name>)
              # manipulate your streaming data
              .writeStream
              .format("mongodb")
              .option("checkpointLocation", "/tmp/pyspark/")
              .option("forceDeleteTempCheckpointLocation", "true")
              .option("spark.mongodb.connection.uri", <mongodb-connection-string>)
              .option("spark.mongodb.database", <database-name>)
              .option("spark.mongodb.collection", <collection-name>)
              .outputMode("append")
            )

            # run the query
            query = dataStreamWriter.start()

     - id: scala
       content: |

         1. Create a 
            `DataStreamReader <https://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/streaming/DataStreamReader.html>`__ 
            that reads from the CSV file.

         #. Create a 
            `DataStreamWriter <https://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/streaming/DataStreamWriter.html>`__
            by calling the ``writeStream()`` method on the streaming 
            Dataset or streaming DataFrame that you created with a 
            ``DataStreamReader``. Specify the format ``mongodb`` using 
            the ``format()`` method.

         #. Call the ``start()`` method on the ``DataStreamWriter``
            instance to begin the stream.
         
         As the connector reads data from the CSV file, it adds that 
         data to MongoDB using the `outputMode <https://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/streaming/DataStreamWriter.html#outputMode(outputMode:String):org.apache.spark.sql.streaming.DataStreamWriter[T]>`__ 
         you specify.

         .. code-block:: scala
            :copyable: true

            // create a local SparkSession
            val spark = SparkSession.builder
              .appName("writeExample")
              .master("spark://spark-master:<port>")
              .config("spark.jars", "<mongo-spark-connector-JAR-file-name>")
              .getOrCreate()

            // define a streaming query
            val dataStreamWriter = spark.readStream
              .format("csv")
              .option("header", "true")
              .schema(<csv-schema>)
              .load(<csv-file-name>)
              // manipulate your streaming data
              .writeStream
              .format("mongodb")
              .option("checkpointLocation", "/tmp/")
              .option("forceDeleteTempCheckpointLocation", "true")
              .option("spark.mongodb.connection.uri", <mongodb-connection-string>)
              .option("spark.mongodb.database", <database-name>)
              .option("spark.mongodb.collection", <collection-name>)
              .outputMode("append")

            // run the query
            val query = dataStreamWriter.start()

Stream to your Console from MongoDB
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

To stream data from MongoDB to your console:

.. tabs-drivers::

   tabs:

     - id: java-sync
       content: |

         1. Create a 
            `DataStreamReader <https://spark.apache.org/docs/latest/api/java/org/apache/spark/sql/streaming/DataStreamReader.html>`__ 
            that reads from MongoDB.

         #. Create a 
            `DataStreamWriter <https://spark.apache.org/docs/latest/api/java/org/apache/spark/sql/streaming/DataStreamWriter.html>`__
            by calling the ``writeStream()`` method on the streaming 
            Dataset or streaming DataFrame that you created with a 
            ``DataStreamReader``. Specify the format ``console`` using 
            the ``format()`` method.

         #. Call the ``start()`` method on the ``DataStreamWriter``
            instance to begin the stream.

         As new data is inserted into MongoDB, MongoDB streams that 
         data out to your console using the `outputMode <https://spark.apache.org/docs/latest/api/java/org/apache/spark/sql/streaming/DataStreamWriter.html#outputMode-java.lang.String->`__ 
         you specify.

         .. include:: /includes/warn-console-stream.txt

         .. code-block:: java
            :copyable: true

            // create a local SparkSession
            SparkSession spark = SparkSession.builder()
              .appName("readExample")
              .master("spark://spark-master:<port>")
              .config("spark.jars", "<mongo-spark-connector-JAR-file-name>")
              .getOrCreate();
             
            // define the schema of the source collection
            StructType readSchema = new StructType()
              .add("company_symbol", DataTypes.StringType)
              .add("company_name", DataTypes.StringType)
              .add("price", DataTypes.DoubleType)
              .add("tx_time", DataTypes.TimestampType);
             
            // define a streaming query
            DataStreamWriter<Row> dataStreamWriter = spark.readStream()
              .format("mongodb")
              .option("spark.mongodb.connection.uri", "<mongodb-connection-string>")
              .option("spark.mongodb.database", "<database-name>")
              .option("spark.mongodb.collection", "<collection-name>")
              .schema(readSchema)
              .load()
              // manipulate your streaming data
              .writeStream()
              .format("console")
              .trigger(Trigger.Continuous("1 second"))
              .outputMode("append");
             
            // run the query
            StreamingQuery query = dataStreamWriter.start();

     - id: python
       content: |

         1. Create a 
            `DataStreamReader <https://spark.apache.org/docs/latest/api/python/reference/pyspark.ss/api/pyspark.sql.streaming.DataStreamReader.html>`__ 
            that reads from MongoDB.

         #. Create a 
            `DataStreamWriter <https://spark.apache.org/docs/latest/api/python/reference/pyspark.ss/api/pyspark.sql.streaming.DataStreamWriter.html>`__
            by calling the ``writeStream()`` method on the streaming 
            DataFrame that you created with a ``DataStreamReader``. 
            Specify the format ``console`` using the ``format()`` method.

         #. Call the ``start()`` method on the ``DataStreamWriter``
            instance to begin the stream.
         
         As new data is inserted into MongoDB, MongoDB streams that 
         data out to your console using the `outputMode <https://spark.apache.org/docs/latest/api/python/reference/pyspark.ss/api/pyspark.sql.streaming.DataStreamWriter.outputMode.html#pyspark.sql.streaming.DataStreamWriter.outputMode>`__ 
         you specify.

         .. include:: /includes/warn-console-stream.txt

         .. code-block:: python
            :copyable: true

            # create a local SparkSession
            spark = SparkSession.builder \
              .appName("readExample") \
              .master("spark://spark-master:<port>") \
              .config("spark.jars", "<mongo-spark-connector-JAR-file-name>") \
              .getOrCreate()

            # define the schema of the source collection
            readSchema = (StructType()
              .add('company_symbol', StringType())
              .add('company_name', StringType())
              .add('price', DoubleType())
              .add('tx_time', TimestampType())
            )

            # define a streaming query
            dataStreamWriter = (spark.readStream
              .format("mongodb")
              .option("spark.mongodb.connection.uri", <mongodb-connection-string>)
              .option('spark.mongodb.database', <database-name>)
              .option('spark.mongodb.collection', <collection-name>)
              .schema(readSchema)
              .load()
              # manipulate your streaming data
              .writeStream
              .format("console")
              .trigger(continuous="1 second")
              .outputMode("append")
            )

            # run the query
            query = dataStreamWriter.start()  

     - id: scala
       content: |

         1. Create a 
            `DataStreamReader <https://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/streaming/DataStreamReader.html>`__ 
            that reads from MongoDB.

         #. Create a 
            `DataStreamWriter <https://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/streaming/DataStreamWriter.html>`__
            by calling the ``writeStream()`` method on the streaming 
            Dataset or streaming DataFrame that you created with a 
            ``DataStreamReader``. Specify the format ``console`` using 
            the ``format()`` method.

         #. Call the ``start()`` method on the ``DataStreamWriter``
            instance to begin the stream.
         
         As new data is inserted into MongoDB, MongoDB streams that 
         data out to your console using the `outputMode <https://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/streaming/DataStreamWriter.html#outputMode(outputMode:String):org.apache.spark.sql.streaming.DataStreamWriter[T]>`__ 
         you specify.

         .. include:: /includes/warn-console-stream.txt

         .. code-block:: scala
            :copyable: true

            // create a local SparkSession
            val spark = SparkSession.builder
              .appName("readExample")
              .master("spark://spark-master:<port>")
              .config("spark.jars", "<mongo-spark-connector-JAR-file-name>")
              .getOrCreate()

            // define the schema of the source collection
            val readSchema = StructType()
              .add("company_symbol", StringType())
              .add("company_name", StringType())
              .add("price", DoubleType())
              .add("tx_time", TimestampType())

            // define a streaming query
            val dataStreamWriter = spark.readStream
              .format("mongodb")
              .option("spark.mongodb.connection.uri", <mongodb-connection-string>)
              .option("spark.mongodb.database", <database-name>)
              .option("spark.mongodb.collection", <collection-name>)
              .schema(readSchema)
              .load()
              // manipulate your streaming data
              .writeStream
              .format("console")
              .trigger(Trigger.Continuous("1 second"))
              .outputMode("append")

            // run the query
            val query = dataStreamWriter.start()
