.. _spark-structured-streaming:

=======================================
Spark Structured Streaming with MongoDB
=======================================

.. default-domain:: mongodb

.. contents:: On this page
   :local:
   :backlinks: none
   :depth: 1
   :class: singlecol

.. todo find a place for /scala/streaming.txt

.. include:: includes/streaming-distinction.rst

Overview
--------

Spark Structured Streaming 

.. Add support for using MongoDB as a streaming source via change streams.
.. https://docs.mongodb.com/kafka-connector/current/source-connector/fundamentals/change-streams/

Spark Structured Streaming is a stream processing engine you can use through the Dataset or DataFrame API.

Spark Structured Streaming allows real-time analysis of live data streams with MongoDB. See the `Spark documentation
<https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html>`__ for a detailed description of Spark Structured Streaming functionality.

.. The key idea in Structured Streaming is to treat a live data stream as a table that is being continuously appended.

.. blog
Apache Spark comes with a stream processing engine called Structured Streaming, which is based on Spark's SQL engine and DataFrame APIs. Spark Structured Streaming treats each incoming stream of data as a micro-batch, continually appending each micro-batch to the target dataset. This makes it easy to  convert existing Spark batch jobs into a streaming job. Structured Streaming provides maximum throughput via the same distributed capabilities that has made Spark such a popular platform.  In the following example, weâ€™ll show you how to stream data to MongoDB using Structured Stream.

While utilizing Spark, the MongoDB Spark Connector also takes advantage of the following MongoDB specific features: 
Aggregation pipelines
Secondary index extraction
Working with only the data it needs (from filtering)  

.. _continuous-processing:

Continuous Processing
~~~~~~~~~~~~~~~~~~~~~

Streaming from a MongoDB database requires *continuous processing*, 
an experimental feature introduced in Spark version 2.3. To learn 
more, see the `Spark documentation <https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#continuous-processing>`__.

Reading a stream from MongoDB with ``.readStream`` depends 
upon watching :manual:`change streams </changeStreams>` for 
real-time updates. 

.. Configuration options can mimic Kafka streaming support. 
.. https://docs.mongodb.com/kafka-connector/current/source-connector/configuration-properties/

.. _read-structured-stream:

Read Stream Configuration
-------------------------

.. note:: 
   
   For general configuration options for the MongoDB Spark Connector, 
   see :ref:`spark-input-conf` and :ref:`spark-output-conf`.

Specify ``.readStream`` configuration settings on your local 
SparkSession.

.. code-block:: python
   :copyable: true

   <SparkSession>
    .readStream.format("csv")
    .option("header", "true")
    .schema(exampleSchema)
    .load("example*.csv")

.. _write-structured-stream:

Write Stream Configuration
--------------------------

.. note:: 
   
   For general configuration options for the MongoDB Spark Connector, 
   see :ref:`spark-input-conf` and :ref:`spark-output-conf`.

Specify ``.writeStream`` configuration settings on your streaming 
DataFrame.

.. code-block:: python
   :copyable: true

   <streaming DataFrame>
    .writeStream
    .format("mongodb")
    .queryName("exampleQuery")
    .option("checkpointLocation", "/tmp/checkpoint/")
    .option("forceDeleteTempCheckpointLocation", "true")
    .option('spark.mongodb.connection.uri', 'mongodb://mongo1,mongo2,mongo3/exampleDatabase.exampleCollection?replicaSet=rs0')
    .outputMode("complete")

Usage Examples
--------------

.. tabs-selector:: drivers

.. tabs-drivers::

   tabs:
     - id: java-sync
       content: |

     - id: python
       content: |

     - id: scala
       content: |

The following examples use a Docker environment that provides a 
`Jupyter <https://jupyter.org/>`__ notebook running 
`pyspark <https://spark.apache.org/docs/latest/api/python/>`__ and a 
MongoDB replica set. You can download and run this environment from 
this `MongoDB University GitHub repository <https://github.com/mongodb-university/mongodb-spark>`__.

Create a ``SparkSession`` configured with the MongoDB Connector to use 
for the following examples:

.. code-block:: python
   :copyable: true

   spark = SparkSession.\
        builder.\
        master("spark://spark-master:7077").\
        config('spark.mongodb.connection.uri', "mongodb://mongo1,mongo2,mongo3/Stocks.Source?replicaSet=rs0").\
        config("spark.jars","mongosparkv10.jar").\
        getOrCreate()

Stream to MongoDB from a CSV file
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

To create a :ref:`write stream <write-structured-stream>` to MongoDB 
from a ``.csv`` file, ...

.. tabs-drivers::

   tabs:
     - id: java-sync
       content: |

         .. code-block:: java
            :copyable: true

     - id: python
       content: |

         .. code-block:: python
            :copyable: true

     - id: scala
       content: |

         .. code-block:: scala
            :copyable: true


Stream to a CSV from MongoDB
~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. note::

   Streaming from MongoDB requires 
   :ref:`continuous processing <continuous-processing>` 
   and an incoming stream of data to write out.

To create a :ref:`read stream <read-structured-stream>` to a ``.csv`` 
file from MongoDB, 

.. tabs-drivers::

   tabs:
     - id: java-sync
       content: |

         .. code-block:: java
            :copyable: true

     - id: python
       content: |

         .. code-block:: python
            :copyable: true

     - id: scala
       content: |

         .. code-block:: scala
            :copyable: true
