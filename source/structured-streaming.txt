.. _spark-structured-streaming:

=================================
Structured Streaming with MongoDB
=================================

.. default-domain:: mongodb

.. contents:: On this page
   :local:
   :backlinks: none
   :depth: 2
   :class: singlecol

Overview
--------

Spark Structured Streaming is a data stream processing engine you can 
use through the Dataset or DataFrame API. The MongoDB Spark Connector 
enables you to stream to and from MongoDB using Spark Structured 
Streaming.

.. include:: includes/streaming-distinction.rst

To learn more about Structured Streaming, see the 
`Spark Programming Guide
<https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html>`__.

.. _write-structured-stream:

Configuring a Write Stream to MongoDB
-------------------------------------

.. tabs-selector:: drivers

.. tabs-drivers::

   tabs:

     - id: java-sync
       content: |

         Specify write stream configuration settings on your streaming 
         Dataset or streaming DataFrame using `DataStreamWriter <https://spark.apache.org/docs/latest/api/java/org/apache/spark/sql/streaming/DataStreamWriter.html>`__. You must specify the following configuration 
         settings to write to MongoDB:
         
         .. list-table::
            :header-rows: 1
            :stub-columns: 1
            :widths: 10 40
         
            * - Setting
              - Description
         
            * - ``writeStream.format()``
              - The format to use for write stream data. Use 
                ``mongodb``.
         
            * - ``writeStream.option()``
              - Use the ``option()`` method to specify your MongoDB 
                deployment :manual:`connection string </reference/connection-string/>` with the 
                ``spark.mongodb.connection.uri`` option key.

                You can also use the ``option()`` method to configure other stream settings, 
                including the MongoDB database and collection, destination 
                directory, and `checkpoint directory <https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#recovering-from-failures-with-checkpointing>`__.
         
            * - ``writeStream.outputMode()``
              - Specifies how data of a streaming DataFrame/Dataset is 
                written to a streaming sink. To view a list of all 
                supported output modes, see `the Java outputMode documentation <https://spark.apache.org/docs/latest/api/java/org/apache/spark/sql/streaming/DataStreamWriter.html#outputMode-java.lang.String->`__.
         
         The following code snippet shows how to use the preceding 
         configuration settings to stream data to MongoDB:

         .. code-block:: java
            :copyable: true
         
            <streaming Dataset/DataFrame>.writeStream()
              .format("mongodb")
              .option("spark.mongodb.connection.uri", <mongodb-connection-string>)
              .option("spark.mongodb.database", <database-name>)
              .option("spark.mongodb.collection", <collection-name>)
              .outputMode("append");

         For a complete list of methods, see the 
         `Java Structured Streaming reference <https://spark.apache.org/docs/latest/api/java/org/apache/spark/sql/streaming/DataStreamWriter.html>`__. 

     - id: python
       content: |

         Specify write stream configuration settings on your streaming 
         DataFrame using `DataStreamWriter <https://spark.apache.org/docs/latest/api/python/reference/pyspark.ss/api/pyspark.sql.streaming.DataStreamWriter.html>`__. You 
         must specify the following configuration settings to write 
         to MongoDB:
         
         .. list-table::
            :header-rows: 1
            :stub-columns: 1
            :widths: 10 40
         
            * - Setting
              - Description
         
            * - ``writeStream.format()``
              - The format to use for write stream data. Use 
                ``mongodb``.
         
            * - ``writeStream.option()``
              - Use the ``option`` method to specify your MongoDB 
                deployment :manual:`connection string </reference/connection-string/>` with the 
                ``spark.mongodb.connection.uri`` option key.
         
                You can also use the ``option()`` method to configure other stream settings, 
                including the MongoDB database and collection, destination 
                directory, and `checkpoint directory <https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#recovering-from-failures-with-checkpointing>`__.

            * - ``writeStream.outputMode()``
              - Specifies how data of a streaming DataFrame is 
                written to a streaming sink. To view a list of all 
                supported output modes, see `the pyspark outputMode documentation <https://spark.apache.org/docs/latest/api/python/reference/pyspark.ss/api/pyspark.sql.streaming.DataStreamWriter.outputMode.html#pyspark.sql.streaming.DataStreamWriter.outputMode>`__.

         
         The following code snippet shows how to use the preceding 
         configuration settings to stream data to MongoDB:

         .. code-block:: python
            :copyable: true
         
            <streaming DataFrame>.writeStream \
              .format("mongodb") \
              .option("spark.mongodb.connection.uri", <mongodb-connection-string>) \
              .option("spark.mongodb.database", <database-name>) \
              .option("spark.mongodb.collection", <collection-name>) \
              .outputMode("append")
         
         For a complete list of methods, see the 
         `pyspark Structured Streaming reference <https://spark.apache.org/docs/latest/api/python/reference/pyspark.ss/api/pyspark.sql.streaming.DataStreamWriter.html>`__.

     - id: scala
       content: |

         Specify write stream configuration settings on your streaming 
         Dataset or streaming DataFrame using `DataStreamWriter <https://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/streaming/DataStreamWriter.html>`__. You must specify the following configuration 
         settings to write to MongoDB:
         
         .. list-table::
            :header-rows: 1
            :stub-columns: 1
            :widths: 10 40
         
            * - Setting
              - Description
         
            * - ``writeStream.format()``
              - The format to use for write stream data. Use 
                ``mongodb``.
         
            * - ``writeStream.option()``
              - Use the ``option`` method to specify your MongoDB 
                deployment :manual:`connection string </reference/connection-string/>` with the 
                ``spark.mongodb.connection.uri`` option key.
         
                You can also use the ``option()`` method to configure other stream settings, 
                including the MongoDB database and collection, destination 
                directory, and `checkpoint directory <https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#recovering-from-failures-with-checkpointing>`__.

            * - ``writeStream.outputMode()``
              - Specifies how data of a streaming DataFrame/Dataset is 
                written to a streaming sink. To view a list of all 
                supported output modes, see `the Scala outputMode documentation <https://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/streaming/DataStreamWriter.html#outputMode(outputMode:String):org.apache.spark.sql.streaming.DataStreamWriter[T]>`__.
         
         The following code snippet shows how to use the preceding 
         configuration settings to stream data to MongoDB:

         .. code-block:: scala
            :copyable: true
         
            <streaming Dataset/DataFrame>.writeStream
              .format("mongodb")
              .option("spark.mongodb.connection.uri", <mongodb-connection-string>)
              .option("spark.mongodb.database", <database-name>)
              .option("spark.mongodb.collection", <collection-name>)
              .outputMode("append")
 
         For a complete list of methods, see the 
         `Scala Structured Streaming reference <https://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/streaming/DataStreamWriter.html>`__.

.. _read-structured-stream:
.. _continuous-processing:

Configuring a Read Stream from MongoDB
--------------------------------------
When reading a stream from a MongoDB database, the {+connector-long+} supports both 
*micro-batch processing* and 
*continuous processing*. Micro-batch processing is the default processing engine, while
continuous processing is an experimental feature introduced in 
Spark version 2.3. To learn 
more about continuous processing, see the `Spark documentation <https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#continuous-processing>`__.

.. include:: /includes/fact-read-from-change-stream

.. note::

    Since Structured Streaming produces a single partition, it ignores
    :ref:`partitioner configurations <partitioner-conf>`. Partitioner
    configuration only apply when there are multiple partitions. This is true
    for both micro-batch processing and continuous processing streams.

.. tabs-drivers::

   tabs:

     - id: java-sync
       content: |

         To read data from MongoDB, specify the following read-stream configuration settings on 
         `DataStreamReader <https://spark.apache.org/docs/latest/api/java/org/apache/spark/sql/streaming/DataStreamReader.html>`__: 
         
         .. list-table::
            :header-rows: 1
            :stub-columns: 1
            :widths: 10 40
         
            * - Setting
              - Description
         
            * - ``readStream.format()``
              - The format to use for read stream data. Use ``mongodb``.
         
            * - ``writeStream.trigger()``
              - Specifies how often results should be 
                written to the streaming sink. 
                
                To use continuous processing, pass ``Trigger.Continuous(<time value>)`` 
                as an argument, where ``<time value>`` is how often the Spark Connector 
                should asynchronously checkpoint. If you 
                pass any other static method of the ``Trigger`` class, or if you don't 
                call ``writeStream.trigger()``, the Spark connector will use 
                micro-batch processing instead. 
  
                To view a list of all supported processing policies, see `the Java 
                trigger documentation <https://spark.apache.org/docs/latest/api/java/org/apache/spark/sql/streaming/Trigger.html>`__.

                .. include:: /includes/note-trigger-method

         The following code snippet shows how to use the preceding 
         configuration settings to continuously process data streamed from MongoDB.
         The connector appends all new data to the existing data and asynchronously 
         writes checkpoints to ``/tmp/checkpointDir`` once per second:

         .. code-block:: java
            :copyable: true
            :emphasize-lines: 1, 4, 8, 13

            import org.apache.spark.sql.streaming.Trigger;
 
            Dataset<Row> streamingDataset = <local SparkSession>.readStream()
              .format("mongodb")
              .load();
 
            DataStreamWriter<Row> dataStreamWriter = streamingDataset.writeStream()
              .trigger(Trigger.Continuous("1 second"))
              .format("memory")
              .option("checkpointLocation", "/tmp/checkpointDir")
              .outputMode("append");
 
            StreamingQuery query = dataStreamWriter.start();

         .. note::
            
            Spark does not begin streaming until you call the 
            ``start()`` method on a streaming query.

         For a complete list of methods, see the 
         `Java Structured Streaming reference <https://spark.apache.org/docs/latest/api/java/org/apache/spark/sql/streaming/DataStreamReader.html>`__.

     - id: python
       content: |

         To read data from MongoDB, specify the following read-stream configuration settings on 
         `DataStreamReader <https://spark.apache.org/docs/latest/api/python/reference/pyspark.ss/api/pyspark.sql.streaming.DataStreamReader.html>`__:  

         To use continuous processing with the MongoDB Spark Connector, 
         call the ``trigger()`` method on the ``writeStream`` property 
         of the streaming DataFrame that you create from 
         your MongoDB read stream. In your ``trigger()``, specify the 
         ``continuous`` parameter.
         
         .. list-table::
            :header-rows: 1
            :stub-columns: 1
            :widths: 10 40
         
            * - Setting
              - Description
         
            * - ``readStream.format()``
              - The format to use for read stream data. Use ``mongodb``.
         
            * - ``writeStream.trigger()``
              - Specifies how often results should be 
                written to the streaming sink. 

                To use continuous processing, pass the method a time value 
                using the ``continuous`` parameter.
                If you pass any other named parameter, or if you don't 
                call ``writeStream.trigger()``, the Spark Connector will use 
                micro-batch processing instead. 
                
                To view a list of all supported processing policies, see 
                `the pyspark trigger documentation <https://spark.apache.org/docs/latest/api/python/reference/pyspark.ss/api/pyspark.sql.streaming.DataStreamWriter.trigger.html>`__.

                .. include:: /includes/note-trigger-method

         The following code snippet shows how to use the preceding 
         configuration settings to continuously process data streamed from MongoDB.
         The connector appends all new data to the existing data and asynchronously 
         writes checkpoints to ``/tmp/checkpointDir`` once per second:

         .. code-block:: python
            :copyable: true
            :emphasize-lines: 2, 7, 13
         
            streamingDataFrame = (<local SparkSession>.readStream
              .format("mongodb")
              .load()
            )
         
            dataStreamWriter = (streamingDataFrame.writeStream
              .trigger(continuous="1 second")
              .format("memory")
              .option("checkpointLocation", "/tmp/checkpointDir")
              .outputMode("append")
            )

            query = dataStreamWriter.start()

         .. note::
            
            Spark does not begin streaming until you call the 
            ``start()`` method on a streaming query.

         For a complete list of methods, see the 
         `pyspark Structured Streaming reference <https://spark.apache.org/docs/latest/api/python/reference/pyspark.ss/api/pyspark.sql.streaming.DataStreamReader.html>`__.

     - id: scala
       content: |
         
         To read data from MongoDB, specify the following read-stream configuration settings on 
         `DataStreamReader <https://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/streaming/DataStreamReader.html>`__:  

         .. list-table::
            :header-rows: 1
            :stub-columns: 1
            :widths: 10 40
         
            * - Setting
              - Description
         
            * - ``readStream.format()``
              - The format to use for read stream data. Use ``mongodb``.
         
            * - ``writeStream.trigger()``
              - Specifies how often results should be 
                written to the streaming sink.

                To use continuous processing, pass ``Trigger.Continuous(<time value>)`` 
                as an argument, where ``<time value>`` is how often the Spark Connector 
                should asynchronously checkpoint. If you 
                pass any other static method of the ``Trigger`` class, or if you don't 
                call ``writeStream.trigger()``, the Spark connector will use 
                micro-batch processing instead. 
                
                To view a list of all 
                supported processing policies, see `the Scala trigger documentation <https://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/streaming/DataStreamWriter.html#trigger(trigger:org.apache.spark.sql.streaming.Trigger):org.apache.spark.sql.streaming.DataStreamWriter[T]>`__.

                .. include:: /includes/note-trigger-method

         The following code snippet shows how to use the preceding 
         configuration settings to continuously process data streamed from MongoDB.
         The connector appends all new data to the existing data and asynchronously 
         writes checkpoints to ``/tmp/checkpointDir`` once per second:

         .. code-block:: scala
            :copyable: true
            :emphasize-lines: 1, 4, 8, 13

            import org.apache.spark.sql.streaming.Trigger
         
            val streamingDataFrame = <local SparkSession>.readStream
              .format("mongodb")
              .load()
         
            val dataStreamWriter = streamingDataFrame.writeStream
              .trigger(Trigger.Continuous("1 second"))
              .format("memory")
              .option("checkpointLocation", "/tmp/checkpointDir")
              .outputMode("append")

            val query = dataStreamWriter.start()

         .. note::
            
            Spark does not begin streaming until you call the 
            ``start()`` method on a streaming query.

         For a complete list of methods, see the 
         `Scala Structured Streaming reference <https://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/streaming/DataStreamReader.html>`__.

Examples
--------

The following examples show Spark Structured Streaming configurations 
for streaming to and from MongoDB.

Stream to MongoDB from a CSV File
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

To stream data from a CSV file to MongoDB:

.. tabs-drivers::

   tabs:

     - id: java-sync
       content: |

         1. Create a 
            `DataStreamReader <https://spark.apache.org/docs/latest/api/java/org/apache/spark/sql/streaming/DataStreamReader.html>`__ 
            that reads from the CSV file.

         #. Create a 
            `DataStreamWriter <https://spark.apache.org/docs/latest/api/java/org/apache/spark/sql/streaming/DataStreamWriter.html>`__ 
            by calling the ``writeStream()`` method on the streaming 
            Dataset or streaming DataFrame that you created with a 
            ``DataStreamReader``. Specify the format ``mongodb`` using 
            the ``format()`` method.

         #. Call the ``start()`` method on the ``DataStreamWriter``
            instance to begin the stream.
         
         As the connector reads data from the CSV file, it adds that 
         data to MongoDB using the `outputMode <https://spark.apache.org/docs/latest/api/java/org/apache/spark/sql/streaming/DataStreamWriter.html#outputMode-java.lang.String->`__ 
         you specify.

         .. code-block:: java
            :copyable: true

            // create a local SparkSession
            SparkSession spark = SparkSession.builder()
              .appName("writeExample")
              .master("spark://spark-master:<port>")
              .config("spark.jars", "<mongo-spark-connector-JAR-file-name>")
              .getOrCreate();
             
            // define a streaming query
            DataStreamWriter<Row> dataStreamWriter = spark.readStream()
              .format("csv")
              .option("header", "true")
              .schema("<csv-schema>")
              .load("<csv-file-name>")
              // manipulate your streaming data
              .writeStream()
              .format("mongodb")
              .option("checkpointLocation", "/tmp/")
              .option("forceDeleteTempCheckpointLocation", "true")
              .option("spark.mongodb.connection.uri", "<mongodb-connection-string>")
              .option("spark.mongodb.database", "<database-name>")
              .option("spark.mongodb.collection", "<collection-name>")
              .outputMode("append");
             
            // run the query
            StreamingQuery query = dataStreamWriter.start();

     - id: python
       content: |

         1. Create a 
            `DataStreamReader <https://spark.apache.org/docs/latest/api/python/reference/pyspark.ss/api/pyspark.sql.streaming.DataStreamReader.html>`__ 
            that reads from the CSV file.

         #. Create a 
            `DataStreamWriter <https://spark.apache.org/docs/latest/api/python/reference/pyspark.ss/api/pyspark.sql.streaming.DataStreamWriter.html>`__
            by calling the ``writeStream()`` method on the streaming 
            DataFrame that you created with a ``DataStreamReader``. 
            Specify the format ``mongodb`` using the ``format()`` method.

         #. Call the ``start()`` method on the ``DataStreamWriter``
            instance to begin the stream.
         
         As the connector reads data from the CSV file, it adds that 
         data to MongoDB using the `outputMode <https://spark.apache.org/docs/latest/api/python/reference/pyspark.ss/api/pyspark.sql.streaming.DataStreamWriter.outputMode.html#pyspark.sql.streaming.DataStreamWriter.outputMode>`__ 
         you specify.

         .. code-block:: python
            :copyable: true

            # create a local SparkSession
            spark = SparkSession.builder \
              .appName("writeExample") \
              .master("spark://spark-master:<port>") \
              .config("spark.jars", "<mongo-spark-connector-JAR-file-name>") \
              .getOrCreate()

            # define a streaming query
            dataStreamWriter = (spark.readStream
              .format("csv")
              .option("header", "true")
              .schema(<csv-schema>)
              .load(<csv-file-name>)
              # manipulate your streaming data
              .writeStream
              .format("mongodb")
              .option("checkpointLocation", "/tmp/pyspark/")
              .option("forceDeleteTempCheckpointLocation", "true")
              .option("spark.mongodb.connection.uri", <mongodb-connection-string>)
              .option("spark.mongodb.database", <database-name>)
              .option("spark.mongodb.collection", <collection-name>)
              .outputMode("append")
            )

            # run the query
            query = dataStreamWriter.start()

     - id: scala
       content: |

         1. Create a 
            `DataStreamReader <https://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/streaming/DataStreamReader.html>`__ 
            that reads from the CSV file.

         #. Create a 
            `DataStreamWriter <https://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/streaming/DataStreamWriter.html>`__
            by calling the ``writeStream()`` method on the streaming 
            Dataset or streaming DataFrame that you created with a 
            ``DataStreamReader``. Specify the format ``mongodb`` using 
            the ``format()`` method.

         #. Call the ``start()`` method on the ``DataStreamWriter``
            instance to begin the stream.
         
         As the connector reads data from the CSV file, it adds that 
         data to MongoDB using the `outputMode <https://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/streaming/DataStreamWriter.html#outputMode(outputMode:String):org.apache.spark.sql.streaming.DataStreamWriter[T]>`__ 
         you specify.

         .. code-block:: scala
            :copyable: true

            // create a local SparkSession
            val spark = SparkSession.builder
              .appName("writeExample")
              .master("spark://spark-master:<port>")
              .config("spark.jars", "<mongo-spark-connector-JAR-file-name>")
              .getOrCreate()

            // define a streaming query
            val dataStreamWriter = spark.readStream
              .format("csv")
              .option("header", "true")
              .schema(<csv-schema>)
              .load(<csv-file-name>)
              // manipulate your streaming data
              .writeStream
              .format("mongodb")
              .option("checkpointLocation", "/tmp/")
              .option("forceDeleteTempCheckpointLocation", "true")
              .option("spark.mongodb.connection.uri", <mongodb-connection-string>)
              .option("spark.mongodb.database", <database-name>)
              .option("spark.mongodb.collection", <collection-name>)
              .outputMode("append")

            // run the query
            val query = dataStreamWriter.start()

Stream to your Console from MongoDB
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

To stream data from MongoDB to your console:

.. tabs-drivers::

   tabs:

     - id: java-sync
       content: |

         1. Create a 
            `DataStreamReader <https://spark.apache.org/docs/latest/api/java/org/apache/spark/sql/streaming/DataStreamReader.html>`__ 
            that reads from MongoDB.

         #. Create a 
            `DataStreamWriter <https://spark.apache.org/docs/latest/api/java/org/apache/spark/sql/streaming/DataStreamWriter.html>`__
            by calling the ``writeStream()`` method on the streaming 
            Dataset or streaming DataFrame that you created with a 
            ``DataStreamReader``. Specify the format ``console`` using 
            the ``format()`` method.

         #. Call the ``start()`` method on the ``DataStreamWriter``
            instance to begin the stream.

         As new data is inserted into MongoDB, MongoDB streams that 
         data out to your console using the `outputMode <https://spark.apache.org/docs/latest/api/java/org/apache/spark/sql/streaming/DataStreamWriter.html#outputMode-java.lang.String->`__ 
         you specify.

         .. include:: /includes/warn-console-stream.txt

         .. code-block:: java
            :copyable: true

            // create a local SparkSession
            SparkSession spark = SparkSession.builder()
              .appName("readExample")
              .master("spark://spark-master:<port>")
              .config("spark.jars", "<mongo-spark-connector-JAR-file-name>")
              .getOrCreate();
             
            // define the schema of the source collection
            StructType readSchema = new StructType()
              .add("company_symbol", DataTypes.StringType)
              .add("company_name", DataTypes.StringType)
              .add("price", DataTypes.DoubleType)
              .add("tx_time", DataTypes.TimestampType);
             
            // define a streaming query
            DataStreamWriter<Row> dataStreamWriter = spark.readStream()
              .format("mongodb")
              .option("spark.mongodb.connection.uri", "<mongodb-connection-string>")
              .option("spark.mongodb.database", "<database-name>")
              .option("spark.mongodb.collection", "<collection-name>")
              .schema(readSchema)
              .load()
              // manipulate your streaming data
              .writeStream()
              .format("console")
              .trigger(Trigger.Continuous("1 second"))
              .outputMode("append");
             
            // run the query
            StreamingQuery query = dataStreamWriter.start();

     - id: python
       content: |

         1. Create a 
            `DataStreamReader <https://spark.apache.org/docs/latest/api/python/reference/pyspark.ss/api/pyspark.sql.streaming.DataStreamReader.html>`__ 
            that reads from MongoDB.

         #. Create a 
            `DataStreamWriter <https://spark.apache.org/docs/latest/api/python/reference/pyspark.ss/api/pyspark.sql.streaming.DataStreamWriter.html>`__
            by calling the ``writeStream()`` method on the streaming 
            DataFrame that you created with a ``DataStreamReader``. 
            Specify the format ``console`` using the ``format()`` method.

         #. Call the ``start()`` method on the ``DataStreamWriter``
            instance to begin the stream.
         
         As new data is inserted into MongoDB, MongoDB streams that 
         data out to your console using the `outputMode <https://spark.apache.org/docs/latest/api/python/reference/pyspark.ss/api/pyspark.sql.streaming.DataStreamWriter.outputMode.html#pyspark.sql.streaming.DataStreamWriter.outputMode>`__ 
         you specify.

         .. include:: /includes/warn-console-stream.txt

         .. code-block:: python
            :copyable: true

            # create a local SparkSession
            spark = SparkSession.builder \
              .appName("readExample") \
              .master("spark://spark-master:<port>") \
              .config("spark.jars", "<mongo-spark-connector-JAR-file-name>") \
              .getOrCreate()

            # define the schema of the source collection
            readSchema = (StructType()
              .add('company_symbol', StringType())
              .add('company_name', StringType())
              .add('price', DoubleType())
              .add('tx_time', TimestampType())
            )

            # define a streaming query
            dataStreamWriter = (spark.readStream
              .format("mongodb")
              .option("spark.mongodb.connection.uri", <mongodb-connection-string>)
              .option('spark.mongodb.database', <database-name>)
              .option('spark.mongodb.collection', <collection-name>)
              .schema(readSchema)
              .load()
              # manipulate your streaming data
              .writeStream
              .format("console")
              .trigger(continuous="1 second")
              .outputMode("append")
            )

            # run the query
            query = dataStreamWriter.start()  

     - id: scala
       content: |

         1. Create a 
            `DataStreamReader <https://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/streaming/DataStreamReader.html>`__ 
            that reads from MongoDB.

         #. Create a 
            `DataStreamWriter <https://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/streaming/DataStreamWriter.html>`__
            by calling the ``writeStream()`` method on the streaming 
            Dataset or streaming DataFrame that you created with a 
            ``DataStreamReader``. Specify the format ``console`` using 
            the ``format()`` method.

         #. Call the ``start()`` method on the ``DataStreamWriter``
            instance to begin the stream.
         
         As new data is inserted into MongoDB, MongoDB streams that 
         data out to your console using the `outputMode <https://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/streaming/DataStreamWriter.html#outputMode(outputMode:String):org.apache.spark.sql.streaming.DataStreamWriter[T]>`__ 
         you specify.

         .. include:: /includes/warn-console-stream.txt

         .. code-block:: scala
            :copyable: true

            // create a local SparkSession
            val spark = SparkSession.builder
              .appName("readExample")
              .master("spark://spark-master:<port>")
              .config("spark.jars", "<mongo-spark-connector-JAR-file-name>")
              .getOrCreate()

            // define the schema of the source collection
            val readSchema = StructType()
              .add("company_symbol", StringType())
              .add("company_name", StringType())
              .add("price", DoubleType())
              .add("tx_time", TimestampType())

            // define a streaming query
            val dataStreamWriter = spark.readStream
              .format("mongodb")
              .option("spark.mongodb.connection.uri", <mongodb-connection-string>)
              .option("spark.mongodb.database", <database-name>)
              .option("spark.mongodb.collection", <collection-name>)
              .schema(readSchema)
              .load()
              // manipulate your streaming data
              .writeStream
              .format("console")
              .trigger(Trigger.Continuous("1 second"))
              .outputMode("append")

            // run the query
            val query = dataStreamWriter.start()
