.. _spark-structured-streaming:

=================================
Structured Streaming with MongoDB
=================================

.. default-domain:: mongodb

.. contents:: On this page
   :local:
   :backlinks: none
   :depth: 2
   :class: singlecol

Overview
--------

Spark Structured Streaming is a data stream processing engine you can 
use through the Dataset or DataFrame API. The MongoDB Spark Connector 
enables you to stream to and from MongoDB using Spark Structured 
Streaming.

.. include:: includes/streaming-distinction.rst

To learn more about Structured Streaming, see the 
`Spark Programming Guide
<https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html>`__.

.. _write-structured-stream:

Configuring a Write Stream to MongoDB
-------------------------------------

Specify write stream configuration settings on your streaming 
Dataset or DataFrame with ``.writeStream``.

The following configuration settings are required:

.. list-table::
   :header-rows: 1
   :stub-columns: 1
   :widths: 10 30

   * - Setting
     - Description

   * - ``.format()``
     - The format to use for write stream data. If you are streaming to 
       a MongoDB deployment, use "mongodb".

   * - ``.outputMode()``
     - The output mode to use. Defaults to "complete".

.. tabs-drivers::

   tabs:
     - id: java-sync
       content: |

     - id: python
       content: |

         .. code-block:: python
            :copyable: true
         
            <streaming Dataset/ DataFrame> \
              .writeStream \
              .queryName("exampleQuery") \
              .format("mongodb") \
              .option("checkpointLocation", "/tmp/checkpoint/") \
              .option("forceDeleteTempCheckpointLocation", "true") \
              .option("spark.mongodb.connection.uri", <mongodb-connection-string>) \
              .outputMode("append")

         For a complete list of methods, see the 
         `pyspark Structured Streaming reference <https://spark.apache.org/docs/latest/api/python/reference/pyspark.ss.html>`__.

     - id: scala
       content: |

.. _read-structured-stream:
.. _continuous-processing:

Configuring a Read Stream from MongoDB
--------------------------------------

Reading a stream from a MongoDB database requires 
*continuous processing*, 
an experimental feature introduced in Spark version 2.3. To learn 
more about continuous processing, see the `Spark documentation <https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#continuous-processing>`__.

To use continuous processing with the MongoDB Spark Connector, add the 
``.trigger()`` method to the ``.writeStream`` of the streaming 
Dataset or DataFrame that you create from your MongoDB ``.readStream``. 
In your ``.trigger()``, specify the ``continuous`` parameter.

.. note:: 

   The connector populates its read stream from your MongoDB 
   deployment's change stream. To populate your change stream, perform 
   update operations on your database.

   To learn more about change streams, see 
   :manual:`Change Streams </changeStreams>` in the MongoDB manual.

Specify read stream configuration settings on your local 
SparkSession ``.readStream``.

.. tabs-drivers::

   tabs:
     - id: java-sync
       content: |

     - id: python
       content: |

         .. code-block:: python
            :copyable: true
            :emphasize-lines: 9
         
            streamingDataFrame = (<local SparkSession>
              .readStream
              .format("mongodb")
              .load()
            )
         
            query = (streamingDataFrame
              .writeStream
              .trigger(continuous="1 second")
              .queryName("exampleQuery")
              .format("memory")
              .outputMode("append")
            )

            query.start()

         For a complete list of methods, see the 
         `pyspark Structured Streaming reference <https://spark.apache.org/docs/latest/api/python/reference/pyspark.ss.html>`__.

     - id: scala
       content: |

Examples
--------

The following examples show Spark Structured Streaming configurations 
for streaming between MongoDB and a CSV file.

Stream to MongoDB from a CSV File
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. tabs-drivers::

   tabs:
     - id: java-sync
       content: |

         .. code-block:: java
            :copyable: true

     - id: python
       content: |

         To create a :ref:`write stream <write-structured-stream>` to 
         MongoDB from a ``.csv`` file, first create a `DataStreamReader <https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.streaming.DataStreamReader.html>`__ 
         from the ``.csv`` file, then use that ``DataStreamReader`` to 
         create a `DataStreamWriter <https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.streaming.DataStreamWriter.html>`__ 
         to MongoDB. Finally, use the ``.start()`` method to begin the 
         stream.
         
         As streaming data is read from the ``.csv`` file, it is added 
         to MongoDB in the `outputMode <https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.streaming.DataStreamWriter.outputMode.html#pyspark.sql.streaming.DataStreamWriter.outputMode>`__ 
         you specify.

         .. code-block:: python
            :copyable: true
            :emphasize-lines: 18, 24

            # create a local SparkSession
            spark = SparkSession \
              .builder \
              .appName("writeExample") \
              .master("spark://spark-master:<port>") \
              .config("spark.jars", "<mongodb-spark-connector-{+current-version+}>.jar") \
              .getOrCreate()
 
            # define the schema of the .csv
            readSchema = (StructType()
              .add('Type', StringType())
              .add('Date', TimestampType())
              .add('Price', DoubleType())
            )

            # define a streaming query
            query = (spark
              .readStream
              .format("csv")
              .option("header", "true")
              .schema(readSchema)
              .load("daily*.csv")
              # manipulate your streaming data
              .writeStream
              .format("mongodb")
              .option("checkpointLocation", "/tmp/pyspark/")
              .option("forceDeleteTempCheckpointLocation", "true")
              .option("spark.mongodb.connection.uri", <mongodb-connection-string>)
              .option('spark.mongodb.database', <database-name>)
              .option('spark.mongodb.collection', <collection-name>)
              .outputMode("append")
            )

            # run the query
            query.start()
            
     - id: scala
       content: |

         .. code-block:: scala
            :copyable: true

Stream to a CSV File from MongoDB
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. tabs-drivers::

   tabs:
     - id: java-sync
       content: |

         .. code-block:: java
            :copyable: true

     - id: python
       content: |

         To create a :ref:`read stream <read-structured-stream>` to a 
         ``.csv`` file from MongoDB, first create a `DataStreamReader <https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.streaming.DataStreamReader.html>`__ 
         from MongoDB, then use that ``DataStreamReader`` to 
         create a `DataStreamWriter <https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.streaming.DataStreamWriter.html>`__ 
         to a new ``.csv`` file. Finally, use the ``.start()`` method 
         to begin the stream.
         
         As new data is inserted into MongoDB, MongoDB streams that 
         data out to a ``.csv`` file in the `outputMode <https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.streaming.DataStreamWriter.outputMode.html#pyspark.sql.streaming.DataStreamWriter.outputMode>`__ 
         you specify.

         .. code-block:: python
            :copyable: true
            :emphasize-lines: 19, 27, 30

            # create a local SparkSession
            spark = SparkSession \
              .builder \
              .appName("readExample") \
              .master("spark://spark-master:<port>") \
              .config("spark.jars", "<mongodb-spark-connector-{+current-version+}>.jar") \
              .getOrCreate()

            # define the schema of the source collection
            readSchema = (StructType()
              .add('company_symbol', StringType())
              .add('company_name', StringType())
              .add('price', DoubleType())
              .add('tx_time', TimestampType())
            )            

            # define a streaming query
            query = (spark
              .readStream
              .format("mongodb")
              .option("spark.mongodb.connection.uri", <mongodb-connection-string>)
              .option('spark.mongodb.database', <database-name>)
              .option('spark.mongodb.collection', <collection-name>)
              .schema(readSchema)
              .load()
              # manipulate your streaming data
              .writeStream
              .format("csv")
              .option("path", "/output/")
              .trigger(continuous="1 second")
              .outputMode("append")
            )

            # run the query
            query.start()   

     - id: scala
       content: |

         .. code-block:: scala
            :copyable: true
