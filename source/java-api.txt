==========================
Spark Connector Java Guide
==========================

.. default-domain:: mongodb

.. admonition:: Source Code

   For the source code that combines all of the Java examples, see
   :mongo-spark:`JavaIntroduction.java
   </blob/master/examples/src/test/java/tour/JavaIntroduction.java>`.

Prerequisites
-------------

.. include:: /includes/list-prerequisites.rst

- Java 8 or later.

Getting Started
---------------

Dependency Management
~~~~~~~~~~~~~~~~~~~~~

.. include:: /includes/scala-java-dependencies.rst

The following excerpt is from a Maven ``pom.xml`` file:

.. code-block:: xml
   
   <dependencies>
     <dependency>
       <groupId>org.mongodb.spark</groupId>
       <artifactId>mongo-spark-connector_2.11</artifactId>
       <version>{+current-version+}</version>
     </dependency>
     <dependency>
       <groupId>org.apache.spark</groupId>
       <artifactId>spark-core_2.11</artifactId>
       <version>{+spark-core-version+}</version>
     </dependency>
     <dependency>
       <groupId>org.apache.spark</groupId>
       <artifactId>spark-sql_2.11</artifactId>
       <version>{+spark-sql-version+}</version>
     </dependency>
   </dependencies>

Configuration
~~~~~~~~~~~~~

For the configuration classes, use the Java-friendly ``create`` methods
instead of the native Scala ``apply`` methods.

The Java API provides a ``JavaSparkContext`` that takes a 
``SparkContext`` object from the ``SparkSession``.

.. include:: /includes/scala-java-sparksession-config.rst

.. code-block:: java

   package com.mongodb.spark_examples;
   
   import org.apache.spark.api.java.JavaSparkContext;
   import org.apache.spark.sql.SparkSession;
   
   public final class GettingStarted {
   
     public static void main(final String[] args) throws InterruptedException {
       /* Create the SparkSession.
        * If config arguments are passed from the command line using --conf,
        * parse args for the values to set.
        */
       SparkSession spark = SparkSession.builder()
         .master("local")
         .appName("MongoSparkConnectorIntro")
         .config("spark.mongodb.input.uri", "mongodb://127.0.0.1/test.myCollection")
         .config("spark.mongodb.output.uri", "mongodb://127.0.0.1/test.myCollection")
         .getOrCreate();
         
       // Create a JavaSparkContext using the SparkSession's SparkContext object
       JavaSparkContext jsc = new JavaSparkContext(spark.sparkContext());
       
       // More application logic would go here...
       
       jsc.close();
         
     }
   }
   
- The :ref:`spark.mongodb.input.uri <spark-input-conf>` specifies the
  MongoDB server  address(``127.0.0.1``), the database to connect
  (``test``), and the collection (``myCollection``) from which to read
  data, and the read preference.

- The :ref:`spark.mongodb.output.uri <spark-output-conf>` specifies the
  MongoDB server address(``127.0.0.1``), the database to connect
  (``test``), and the collection (``myCollection``) to which to write
  data.

You can use a ``SparkSession`` object to write data to MongoDB, read
data from MongoDB, create Datasets, and perform SQL operations.

``MongoSpark`` Helper
~~~~~~~~~~~~~~~~~~~~~

To facilitate interaction between MongoDB and Spark, the MongoDB Spark
Connector provides the ``com.mongodb.spark.api.java.MongoSpark``
helper.

Tutorials
---------

- :doc:`/java/write-to-mongodb`
- :doc:`/java/read-from-mongodb`
- :doc:`/java/aggregation`
- :doc:`/java/datasets-and-sql`

.. class:: hidden

   .. toctree::
      :titlesonly:

      /java/write-to-mongodb
      /java/read-from-mongodb
      /java/aggregation
      /java/datasets-and-sql
