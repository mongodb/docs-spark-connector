.. _spark-write-conf:

===========================
Write Configuration Options
===========================

.. default-domain:: mongodb

.. contents:: On this page
   :local:
   :backlinks: none
   :depth: 1
   :class: singlecol

.. _spark-output-conf:

Write Configuration
-------------------

The following options for writing to MongoDB are available:

.. note::

   If you use ``SparkConf`` to set the connector's write configurations,
   prefix ``spark.mongodb.write.`` to each property.

.. list-table::
   :header-rows: 1
   :widths: 35 65

   * - Property name
     - Description

   * - ``collection``
     - | **Required.**
       | The collection name configuration.

   * - ``connection.uri``
     - | **Required.**
       | The connection string configuration key.
       |
       | **Default:** ``mongodb://localhost:27017/``

   * - ``convertJson``
     - | When ``true``, the connector parses the string and converts extended JSON
         into BSON.
       |
       | **Default:** ``false``

   * - ``database``
     - | **Required.**
       | The database name configuration.

   * - ``idFieldList``
     - | Field or list of fields by which to split the collection data. To
         specify more than one field, separate them using a comma as shown
         in the following example:

       .. code-block:: none
          :copyable: false

          "fieldName1,fieldName2"

       | **Default:** ``_id``

   * - ``ignoreNullValues``
     - | When ``true``, the connector ignores any ``null`` values when writing,
         including ``null`` values in arrays and nested documents.
       |
       | **Default:** ``false``

   * - ``maxBatchSize``
     - | Specifies the maximum number of operations to batch in bulk
         operations.
       |
       | **Default:** ``512``

   * - ``mongoClientFactory``
     - | MongoClientFactory configuration key.
       | You can specify a custom implementation which must implement the
         ``com.mongodb.spark.sql.connector.connection.MongoClientFactory``
         interface.
       |
       | **Default:** ``com.mongodb.spark.sql.connector.connection.DefaultMongoClientFactory``

   * - ``operationType``
     - | Specifies the type of write operation to perform. You can set
         this to one of the following values:

       - ``insert``: insert the data.
       - ``replace``: replace an existing document that matches the
         ``idFieldList`` value with the new data. If no match exists, the
         value of ``upsertDocument`` indicates whether the connector
         inserts a new document.
       - ``update``: update an existing document that matches the
         ``idFieldList`` value with the new data. If no match exists, the
         value of ``upsertDocument`` indicates whether the connector
         inserts a new document.

       |
       | **Default:** ``replace``

   * - ``ordered``
     - | Specifies whether to perform ordered bulk operations.
       |
       | **Default:** ``true``

   * - ``upsertDocument``
     - | When ``true``, replace and update operations will insert the data
         if no match exists.
       |
       | For time series collections, you must set ``upsertDocument`` to
         ``false``.
       |
       | **Default:** ``true``

   * - ``writeConcern.journal``
     - | Specifies ``j``, a write-concern option to enable request for
         acknowledgment that the data is confirmed on on-disk journal for
         the criteria specified in the ``w`` option. You can specify
         either ``true`` or ``false``.
       |
       | For more information on ``j`` values, see the MongoDB server
         guide on the
         :manual:`WriteConcern j option </reference/write-concern/#j-option>`.

   * - ``writeConcern.w``
     - | Specifies ``w``, a write-concern option to request acknowledgment
         that the write operation has propogated to a specified number of
         MongoDB nodes. For a list
         of allowed values for this option, see :manual:`WriteConcern
         </reference/write-concern/#w-option>` in the MongoDB manual.
       |
       | **Default:** ``1``

   * - ``writeConcern.wTimeoutMS``
     - | Specifies ``wTimeoutMS``, a write-concern option to return an error
         when a write operation exceeds the number of milliseconds. If you
         use this optional setting, you must specify a nonnegative integer.
       |
       | For more information on ``wTimeoutMS`` values, see the MongoDB server
         guide on the
         :manual:`WriteConcern wtimeout option </reference/write-concern/#wtimeout>`.

.. _configure-output-uri:

``connection.uri`` Configuration Setting
----------------------------------------

You can set all :ref:`spark-output-conf` via the write ``connection.uri``.

.. note::

   If you use ``SparkConf`` to set the connector's write configurations,
   prefix ``spark.mongodb.write.`` to the setting.

.. code:: cfg

  spark.mongodb.write.connection.uri=mongodb://127.0.0.1/test.myCollection

The configuration corresponds to the following separate configuration
settings:

.. code:: cfg

  spark.mongodb.write.connection.uri=mongodb://127.0.0.1/
   spark.mongodb.write.database=test
   spark.mongodb.write.collection=myCollection

If you specify a setting both in the ``connection.uri`` and in a separate
configuration, the ``connection.uri`` setting overrides the separate
setting. For example, in the following configuration, the
database for the connection is ``foobar``:

.. code:: cfg

  spark.mongodb.write.connection.uri=mongodb://127.0.0.1/foobar
   spark.mongodb.write.database=bar
