.. _spark-read-conf:

==========================
Read Configuration Options
==========================

.. default-domain:: mongodb

.. contents:: On this page
   :local:
   :backlinks: none
   :depth: 1
   :class: singlecol

.. _spark-input-conf:

Read Configuration
------------------

You can configure the following properties to read from MongoDB:

.. note::

   If you use ``SparkConf`` to set the connector's read configurations,
   prefix ``spark.mongodb.read.`` to each property.

.. list-table::
   :header-rows: 1
   :widths: 35 65

   * - Property name
     - Description

   * - ``spark.mongodb.read.``
     - Read configuration prefix.

   * - ``mongoClientFactory``
     - | MongoClientFactory configuration key.
       | You can specify a custom implementation which must implement the
         ``com.mongodb.spark.sql.connector.connection.MongoClientFactory``
         interface.
       |
       | **Default:** ``com.mongodb.spark.sql.connector.connection.DefaultMongoClientFactory``

   * - ``connection.uri``
     - | **Required.**
       | The connection string configuration key.
       |
       | **Default:** ``mongodb://localhost:27017/``

   * - ``database``
     - | **Required.**
       | The database name configuration.

   * - ``collection``
     - | **Required.**
       | The collection name configuration.

   * - ``partitioner``
     - | The partitioner full class name.

       | You can specify a custom implementation which must implement the
         ``com.mongodb.spark.sql.connector.read.partitioner.Partitioner``
          interface.
       |
       | **Default:** ``com.mongodb.spark.sql.connector.read.partitioner.SamplePartitioner``

   * - ``partioner.options.``
     - Partitioner configuration prefix.

   * - ``sampleSize``
     - | The number of documents to sample from the collection when inferring
       | the schema.
       |
       | **Default:** ``1000``

   * - ``sql.inferSchema.mapTypes.enabled``
     - | Whether to enable Map types when inferring the schema.
       | When enabled, large compatible struct types are inferred to a
         ``MapType`` instead.
       |
       | **Default:** ``true``

   * - ``sql.inferSchema.mapTypes.minimum.key.size``
     - | Minimum size of a ``StructType`` before inferring as a ``MapType``.
       |
       | **Default:** ``250``

   * - ``aggregation.pipeline``
     - | Specifies a custom aggregation pipeline to apply to the collection
         before sending data to Spark.
       | The value must be either an extended JSON single document or list
         of documents.
       | A single document should resemble the following:

       .. code-block:: json

          {"$match": {"closed": false}}

       | A list of documents should resemble the following:

       .. code-block:: json

          [{"$match": {"closed": false}}, {"$project": {"status": 1, "name": 1, "description": 1}}]

       .. important::

          Custom aggregation pipelines must be compatible with the
          partitioner strategy. For example, aggregation stages such as
          ``$group`` do not work with any partitioner that creates more than
          one partition.

   * - ``aggregation.allowDiskUse``
     - | Specifies whether to allow storage to disk when running the
         aggregation.
       |
       | **Default:** ``true``

   * - ``change.stream.publish.full.document.only``
     - | Specifies whether to publish the changed document or the full
         change stream document.
       |
       | When set to ``true``, the connector filters out messages that
         omit the ``fullDocument`` field and only publishes the value of the
         field.

       .. note::

          This setting overrides the ``change.stream.lookup.full.document``
          setting.

       |
       | **Default**: ``false``

   * - ``change.stream.lookup.full.document``
     - | Specifies what information to return for update operations when
         using a change stream.
       |
       | When set to ``updateLookup``, the connector returns the most
         current majority-committed version of the updated document.
       |
       | When set to ``default``, the connector returns the server's
         default value for the ``fullDocument`` field.

       |
       | For more information on change stream update operation
         configuration, see the MongoDB server manual guide on
         :manual:`Lookup Full Document for Update Operations </changeStreams/#lookup-full-document-for-update-operations>`
       |
       | **Default**: ``default``


.. _partitioner-conf:

Partitioner Configuration
~~~~~~~~~~~~~~~~~~~~~~~~~

.. _conf-mongosamplepartitioner:
.. _conf-samplepartitioner:

``SamplePartitioner`` Configuration
```````````````````````````````````

.. include:: /includes/sparkconf-partitioner-options-note.rst

.. list-table::
   :header-rows: 1
   :widths: 35 65

   * - Property name
     - Description

   * - ``partitioner.options.partitionKey``
     - A comma-delimited list of fields by which to split the
       collection data. The fields should be indexed and contain unique
       values.

       **Default:** ``_id``

   * - ``partitioner.options.partitionSizeMB``
     - The size (in MB) for each partition. Smaller partition sizes
       create more partitions containing fewer documents.

       **Default:** ``64``

   * - ``partitioner.options.samplesPerPartition``
     - The number of sample documents to take for each partition in
       order to establish a ``partitionKey`` range for each partition.

       A greater number of ``samplesPerPartition`` helps to find
       ``partitionKey`` ranges that more closely match the
       ``partitionSizeMB`` you specify.

       .. note::

          For sampling to improve performance, ``samplesPerPartition``
          must be fewer than the number of documents within each of
          your partitions.

          You can estimate the number of documents within each of your
          partitions by dividing your ``partitionSizeMB`` by the
          average document size (in MB) in your collection.

       **Default:** ``10``

.. example::

   For a collection with 640 documents with an average document
   size of 0.5 MB, the default ``SamplePartitioner`` configuration
   values creates 5 partitions with 128 documents per partition.

   The MongoDB Spark Connector samples 50 documents (the default 10
   per intended partition) and defines 5 partitions by selecting
   ``partitionKey`` ranges from the sampled documents.

.. _conf-mongoshardedpartitioner:
.. _conf-shardedpartitioner:

``ShardedPartitioner`` Configuration
````````````````````````````````````

The ``ShardedPartitioner`` automatically determines
partitions to use based on your shard configuration.

This partitioner is not compatible with hashed shard keys.

.. _conf-mongopaginatebysizepartitioner:
.. _conf-paginatebysizepartitioner:

``PaginateBySizePartitioner`` Configuration
```````````````````````````````````````````

.. include:: /includes/sparkconf-partitioner-options-note.rst

.. list-table::
   :header-rows: 1
   :widths: 35 65

   * - Property name
     - Description

   * - ``partitioner.options.partitionKey``
     - A comma-delimited list of fields by which to split the
       collection data. The fields should be indexed and contain unique
       values.

       **Default:** ``_id``

   * - ``partitioner.options.partitionSizeMB``
     - The size (in MB) for each partition. Smaller partition sizes
       create more partitions containing fewer documents.

       **Default:** ``64``

.. _conf-paginateintopartitionspartitioner:

``PaginateIntoPartitionsPartitioner`` Configuration
```````````````````````````````````````````````````

.. include:: /includes/sparkconf-partitioner-options-note.rst

.. list-table::
   :header-rows: 1
   :widths: 35 65

   * - Property name
     - Description

   * - ``partitioner.options.partitionKey``
     - A comma-delimited list of fields by which to split the
       collection data. The fields should be indexed and contain unique
       values.

       **Default:** ``_id``

   * - ``partitioner.options.maxNumberOfPartitions``
     - The number of partitions to create.

       **Default:** ``64``

.. _spark-change-stream-conf:

Change Streams
--------------

.. note::

   If you use ``SparkConf`` to set the connector's change stream
   configurations, prefix ``spark.mongodb.change.stream.`` to each
   property.

.. list-table::
   :header-rows: 1
   :widths: 35 65

   * - Property name
     - Description

   * - ``lookup.full.document``

     - Determines what values your change stream returns on update
       operations.

       The default setting returns the differences between the original
       document and the updated document.

       The ``updateLookup`` setting returns the differences between the
       original document and updated document as well as a copy of the
       entire updated document.

       .. tip::

          For more information on how this change stream option works,
          see the MongoDB server manual guide
          :manual:`Lookup Full Document for Update Operation </changeStreams/#lookup-full-document-for-update-operations>`.

       **Default:** "default"

   * - ``publish.full.document.only``

     - If ``true``, this property returns only the changed document
       instead of the full change stream document. The connector
       automatically sets the ``lookup.full.document`` property to
       ``updateLookup`` to receive the updated documents.

       **Default:** ``false``

.. _configure-input-uri:

``connection.uri`` Configuration Setting
----------------------------------------

You can set all :ref:`spark-input-conf` via the read ``connection.uri`` setting.

For example, consider the following example which sets the read
``connection.uri`` setting via ``SparkConf``:

.. note::

   If you use ``SparkConf`` to set the connector's read configurations,
   prefix ``spark.mongodb.read.`` to the setting.

.. code:: cfg

   spark.mongodb.read.connection.uri=mongodb://127.0.0.1/databaseName.collectionName?readPreference=primaryPreferred

The configuration corresponds to the following separate configuration
settings:

.. code:: cfg

   spark.mongodb.read.connection.uri=mongodb://127.0.0.1/
   spark.mongodb.read.database=databaseName
   spark.mongodb.read.collection=collectionName
   spark.mongodb.read.readPreference.name=primaryPreferred

If you specify a setting both in the ``connection.uri`` and in a separate
configuration, the ``connection.uri`` setting overrides the separate
setting. For example, given the following configuration, the
database for the connection is ``foobar``:

.. code:: cfg

   spark.mongodb.read.connection.uri=mongodb://127.0.0.1/foobar
   spark.mongodb.read.database=bar
