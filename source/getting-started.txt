===============
Getting Started
===============

.. default-domain:: mongodb

First, select the language in which you want to work with 
the Spark Connector:

.. tabs-drivers::
   .. tab:: Java
      :tabid: java-sync

   .. tab:: Python
      :tabid: python

   .. tab:: Scala
      :tabid: scala

Prerequisites
-------------

.. include:: /includes/list-prerequisites.rst

- Java 8 or later.

.. tabs-drivers::
   :hidden: true

   .. tab::
      :tabid: java-sync

      .. note:: Source Code

         For the source code that combines all of the Java examples, see
         :mongo-spark:`JavaIntroduction.java
         </blob/master/examples/src/test/java/tour/JavaIntroduction.java>`.

      Dependency Management
      ---------------------

      .. include:: /includes/scala-java-dependencies.rst

      The following excerpt is from a Maven ``pom.xml`` file:
      
      .. code-block:: xml
         
         <dependencies>
           <dependency>
             <groupId>org.mongodb.spark</groupId>
             <artifactId>mongo-spark-connector_{+scala-version+}</artifactId>
             <version>{+current-version+}</version>
           </dependency>
           <dependency>
             <groupId>org.apache.spark</groupId>
             <artifactId>spark-core_{+scala-version+}</artifactId>
             <version>{+spark-core-version+}</version>
           </dependency>
           <dependency>
             <groupId>org.apache.spark</groupId>
             <artifactId>spark-sql_{+scala-version+}</artifactId>
             <version>{+spark-sql-version+}</version>
           </dependency>
         </dependencies>

      Configuration
      -------------
      
      For the configuration classes, use the Java-friendly ``create`` methods
      instead of the native Scala ``apply`` methods.
      
      The Java API provides a ``JavaSparkContext`` that takes a 
      ``SparkContext`` object from the ``SparkSession``.
      
      .. include:: /includes/scala-java-sparksession-config.rst
      
      .. code-block:: java
      
         package com.mongodb.spark_examples;
         
         import org.apache.spark.api.java.JavaSparkContext;
         import org.apache.spark.sql.SparkSession;
         
         public final class GettingStarted {
         
           public static void main(final String[] args) throws InterruptedException {
             /* Create the SparkSession.
              * If config arguments are passed from the command line using --conf,
              * parse args for the values to set.
              */
             SparkSession spark = SparkSession.builder()
               .master("local")
               .appName("MongoSparkConnectorIntro")
               .config("spark.mongodb.input.uri", "mongodb://127.0.0.1/test.myCollection")
               .config("spark.mongodb.output.uri", "mongodb://127.0.0.1/test.myCollection")
               .getOrCreate();
               
             // Create a JavaSparkContext using the SparkSession's SparkContext object
             JavaSparkContext jsc = new JavaSparkContext(spark.sparkContext());
             
             // More application logic would go here...
             
             jsc.close();
               
           }
         }
         
      - The :ref:`spark.mongodb.input.uri <spark-input-conf>` specifies 
        the
        MongoDB server  address(``127.0.0.1``), the database to connect
        (``test``), and the collection (``myCollection``) from which to 
        read data, and the read preference.
      
      - The :ref:`spark.mongodb.output.uri <spark-output-conf>` 
        specifies the
        MongoDB server address(``127.0.0.1``), the database to connect
        (``test``), and the collection (``myCollection``) to which to write data.
      
      You can use a ``SparkSession`` object to write data to MongoDB, read
      data from MongoDB, create Datasets, and perform SQL operations.
      
      ``MongoSpark`` Helper
      ~~~~~~~~~~~~~~~~~~~~~
      
      To facilitate interaction between MongoDB and Spark, the MongoDB Spark
      Connector provides the ``com.mongodb.spark.api.java.MongoSpark``
      helper.

   .. tab::
      :tabid: python

      .. note:: Source Code

         For the source code that contains the examples below, see
         :mongo-spark:`introduction.py
         </blob/master/examples/src/test/python/introduction.py>`.

      Python Spark Shell
      ------------------
      
      This tutorial uses the ``pyspark`` shell, but the code works
      with self-contained Python applications as well.
      
      When starting the ``pyspark`` shell, you can specify:
      
      .. include:: /includes/extracts/command-line-start-pyspark.rst
      
      .. _python-basics:
      
      Create a ``SparkSession`` Object
      --------------------------------
      
      .. note:: 
      
         When you start ``pyspark`` you get a ``SparkSession`` object called
         ``spark`` by default. In a standalone Python application, you need
         to create your ``SparkSession`` object explicitly, as show below.
         
      If you specified the ``spark.mongodb.input.uri``
      and ``spark.mongodb.output.uri`` configuration options when you
      started ``pyspark``, the default ``SparkSession`` object uses them.
      If you'd rather create your own ``SparkSession`` object from within
      ``pyspark``, you can use ``SparkSession.builder`` and specify different
      configuration options.
      
      .. code-block:: python
      
         from pyspark.sql import SparkSession
      
         my_spark = SparkSession \
             .builder \
             .appName("myApp") \
             .config("spark.mongodb.input.uri", "mongodb://127.0.0.1/test.coll") \
             .config("spark.mongodb.output.uri", "mongodb://127.0.0.1/test.coll") \
             .getOrCreate()
      
      You can use a ``SparkSession`` object to write data to MongoDB, read
      data from MongoDB, create DataFrames, and perform SQL operations.

   .. tab::
      :tabid: scala

      Spark Shell
      -----------
      
      When starting the Spark shell, specify:
      
      .. include:: /includes/extracts/command-line-start-spark-shell.rst
      
      Import the MongoDB Connector Package
      ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
      
      Enable MongoDB Connector specific functions and implicits for the
      ``SparkSession`` and RDD (Resilient Distributed Dataset) by importing
      the following package in the Spark shell:
      
      .. code-block:: scala
      
         import com.mongodb.spark._
      
      Connect to MongoDB
      ~~~~~~~~~~~~~~~~~~
      
      Connection to MongoDB happens automatically when an RDD action
      requires a :ref:`read <scala-read>` from MongoDB or a
      :ref:`write <scala-write>` to MongoDB.
      
      .. _scala-app:
      
      Self-Contained Scala Application
      --------------------------------
      
      Dependency Management
      ~~~~~~~~~~~~~~~~~~~~~
      
      .. include:: /includes/scala-java-dependencies.rst
      
      The following excerpt demonstrates how to include these dependencies in
      a `SBT <http://www.scala-sbt.org/documentation.html>`_ ``build.scala`` file:
      
      .. code-block:: scala
      
         scalaVersion := "{+scala-version-full+}",
         libraryDependencies ++= Seq(
           "org.mongodb.spark" %% "mongo-spark-connector" % "{+current-version+}",
           "org.apache.spark" %% "spark-core" % "{+spark-core-version+}",
           "org.apache.spark" %% "spark-sql" % "{+spark-sql-version+}"
         )
      
      Configuration
      ~~~~~~~~~~~~~
      
      .. include:: /includes/scala-java-sparksession-config.rst
      
      .. code-block:: scala
      
         package com.mongodb
      
         object GettingStarted {
      
           def main(args: Array[String]): Unit = {
           
             /* Create the SparkSession.
              * If config arguments are passed from the command line using --conf,
              * parse args for the values to set.
              */
             import org.apache.spark.sql.SparkSession
           
             val spark = SparkSession.builder()
               .master("local")
               .appName("MongoSparkConnectorIntro")
               .config("spark.mongodb.input.uri", "mongodb://127.0.0.1/test.myCollection")
               .config("spark.mongodb.output.uri", "mongodb://127.0.0.1/test.myCollection")
               .getOrCreate()
               
           }
         }
      
      MongoSpark Helper
      -----------------
      
      If you require granular control over your configuration, then the
      ``MongoSpark`` companion provides a ``builder()`` method for
      configuring all aspects of the Mongo Spark Connector. It also provides
      methods to create an RDD, ``DataFrame`` or ``Dataset``.
      
      Troubleshooting
      ---------------
      
      If you get a ``java.net.BindException: Can't assign requested address``,
      
      - Check to ensure that you do not have another Spark shell already
        running.
      
      - Try setting the ``SPARK_LOCAL_IP`` environment variable; e.g.
      
        .. code-block:: sh
        
           export SPARK_LOCAL_IP=127.0.0.1
      
      - Try including the following option when starting the Spark shell:
      
        .. code-block:: sh
      
           --driver-java-options "-Djava.net.preferIPv4Stack=true"
      
      If you have errors running the examples in this tutorial, you may need
      to clear your local ivy cache (``~/.ivy2/cache/org.mongodb.spark`` and
      ``~/.ivy2/jars``).

Tutorials
---------

- :doc:`write-to-mongodb`
- :doc:`read-from-mongodb`
- :doc:`streaming`
